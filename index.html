<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-129673183-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-129673183-1');
</script>

  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
      transition: all 0.2s ease;
      position: relative;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    /* Subtle underline effect for links in paragraphs */
    p a:not([href*="mailto"]):not([href*="twitter"]):hover {
      text-decoration: underline;
      text-decoration-color: rgba(240, 146, 40, 0.3);
      text-underline-offset: 2px;
    }
    
    body {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      line-height: 1.6;
      background: linear-gradient(to bottom, #ffffff 0%, #f8fafb 50%, #ffffff 100%);
      background-attachment: fixed;
    }
    
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      line-height: 1.6;
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 24px;
      font-weight: 600;
      color: #2c3e50;
      display: block;
      margin-bottom: 10px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700;
    }
    
    /* Enhanced paper title link styling */
    a papertitle {
      transition: all 0.2s ease;
    }
    
    a:hover papertitle {
      letter-spacing: 0.3px;
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 34px;
      font-weight: 700;
      color: #1a1a1a;
      letter-spacing: -0.5px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
    
    /* Publication row hover effect */
    .publication-row {
      transition: all 0.3s ease;
      border-radius: 8px;
      padding: 10px;
    }
    
    .publication-row:hover {
      transform: translateY(-3px);
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
      background-color: rgba(23, 114, 208, 0.02);
    }
    
    /* Better paragraph spacing */
    p {
      margin-bottom: 12px;
    }
    
    /* Section spacing */
    table[cellpadding="20"] {
      margin-bottom: 5px;
    }
    
    /* List styling */
    ul {
      line-height: 1.8;
    }
    
    li {
      margin-bottom: 8px;
    }
    
    /* News section styling */
    .news-section {
      background-color: rgba(247, 250, 252, 0.8);
      border-radius: 8px;
      padding: 20px;
    }
    
    .news-section ul {
      list-style-type: none;
      padding-left: 0;
    }
    
    .news-section li {
      padding: 10px 15px;
      margin-bottom: 10px;
      border-left: 3px solid #1772d0;
      background-color: white;
      border-radius: 4px;
      transition: all 0.2s ease;
    }
    
    .news-section li:hover {
      transform: translateX(5px);
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
      border-left-color: #f09228;
    }
    
    /* Details/Summary styling for older news */
    details.details-example {
      margin-top: 15px;
    }
    
    details.details-example summary {
      cursor: pointer;
      padding: 10px 15px;
      background-color: rgba(23, 114, 208, 0.05);
      border-radius: 4px;
      font-weight: 600;
      color: #1772d0;
      transition: all 0.2s ease;
    }
    
    details.details-example summary:hover {
      background-color: rgba(23, 114, 208, 0.1);
    }
    
    details.details-example[open] summary {
      margin-bottom: 10px;
    }
    
    /* Institution timeline styling */
    .institution-timeline td:hover {
      background-color: rgba(23, 114, 208, 0.05) !important;
    }
    
    .institution-timeline img {
      transition: all 0.3s ease;
      filter: grayscale(0%);
      opacity: 0.9;
    }
    
    .institution-timeline img:hover {
      filter: grayscale(0%);
      opacity: 1;
      transform: scale(1.05);
    }
    
    /* Publication media styling */
    .publication-row img,
    .publication-row video {
      border-radius: 6px;
      transition: all 0.3s ease;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    }
    
    .publication-row:hover img,
    .publication-row:hover video {
      box-shadow: 0 4px 16px rgba(0, 0, 0, 0.15);
      transform: scale(1.02);
    }
    
    /* Smooth appearance for all images */
    img {
      image-rendering: -webkit-optimize-contrast;
      image-rendering: crisp-edges;
    }
    
    /* Contact links bar styling - Connected segments */
    .contact-links {
      padding: 20px;
      margin-top: 15px;
      display: flex;
      justify-content: center;
      align-items: center;
    }
    
    .contact-links a {
      padding: 10px 20px;
      margin: 0;
      border: 1.5px solid rgba(23, 114, 208, 0.3);
      border-right: none;
      transition: all 0.2s ease;
      display: inline-block;
      background-color: rgba(255, 255, 255, 0.8);
      font-weight: 500;
      color: #1772d0;
    }
    
    .contact-links a:first-of-type {
      border-radius: 8px 0 0 8px;
    }
    
    .contact-links a:last-of-type {
      border-radius: 0 8px 8px 0;
      border-right: 1.5px solid rgba(23, 114, 208, 0.3);
    }
    
    .contact-links a:hover {
      background-color: rgba(23, 114, 208, 0.1);
      color: #f09228 !important;
      z-index: 1;
      position: relative;
    }
    
    /* Profile image styling */
    .profile-image {
      border-radius: 8px;
      box-shadow: 0 4px 16px rgba(0, 0, 0, 0.12);
      border: 1px solid rgba(0, 0, 0, 0.08);
    }
    
    /* Asymmetric intro layout */
    .asymmetric-intro .name-header {
      text-align: center;
      padding-bottom: 20px;
      margin-bottom: 20px;
    }
    
    .asymmetric-intro .content-row {
      display: flex;
      align-items: flex-start;
      gap: 30px;
    }
    
    .asymmetric-intro .bio-content {
      flex: 2;
    }
    
    .asymmetric-intro .image-content {
      flex: 1;
      display: flex;
      align-items: center;
      justify-content: center;
    }
    
    /* Main content container */
    body > table {
      background-color: rgba(255, 255, 255, 0.9);
      box-shadow: 0 0 30px rgba(0, 0, 0, 0.05);
      border-radius: 10px;
      margin-top: 20px;
      margin-bottom: 20px;
      padding: 20px;
    }
    
    /* Section spacing improvements */
    heading {
      margin-top: 10px;
    }
  </style>
  <link rel="icon" type="image/png" href="images/icon.png">
  <title>Ankit Goyal</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <!-- Bio and Image -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" class="asymmetric-intro">
          <tr>
            <td width="100%" class="name-header">
              <name>Ankit Goyal</name>
            </td>
          </tr>
          <tr>
            <td width="100%">
              <div class="content-row">
                <div class="bio-content">
                  <p align="justify">
		    I am a Senior Research Scientist in Robotics at NVIDIA. I did my Ph.D. in Computer Science at Princeton University, where I was advised by <a href = "http://www.cs.princeton.edu/~jiadeng/">Prof. Jia Deng</a>. I completed Masters from University of Michigan and Bachelors from IIT Kanpur. 
	          </p>
	          <!-- <p align="justify" style="color:red;"> -->
	          <!--   <b><span color="red"> --> 
		  <!-- I am looking for full-time research positions starting from Summer/Fall 2022. -->
		  <!-- </span></b> -->
	          <!-- </p> -->
	          <p align="justify"> 
	      	    I have been fortunate to work with some amazing mentors and pioneers in the field of AI and Robotics.
	            <ul>
	              <li> <i>2021-25:</i> <a href="https://homes.cs.washington.edu/~fox/"> Dieter Fox </a> at NVIDIA
	              <li> <i>2018-22:</i> <a href="http://www.cs.princeton.edu/~jiadeng/"> Jia Deng </a> at Princeton University
                <li> <i>2021:</i> <a href="http://vladlen.info/"> Vladlen Koltun </a> at Intel
	              <li> <i>2016:</i> <a href="https://www.prateekjain.org/">Prateek Jain</a> at MSR India
	              <li> <i>2015:</i> <a href="https://sail.usc.edu/people/shri.php">Shrikanth Narayanan</a>, <a href="http://tanayag.com/">Tanaya Guha</a> and <a herf="https://navkr.com/"> Naveen Kumar </a> at USC
	            </ul>
                  </p>
                </div>
                <div class="image-content">
                  <img src="images/self_rectangular.png" class="profile-image" style="width: 100%; max-width: 260px;">
                </div>
              </div>
	    </td>
          </tr>
	  <tr>
            <td colspan="2">
              <p align=center margin=0px class="contact-links">
                <a href="mailto:ankgoyal@umich.edu">Email</a>
                <a href="data/AnkitGoyal_CV.pdf">CV</a>
                <a href="https://scholar.google.com/citations?user=RhN6jKIAAAAJ&hl=en&oi=sra">Scholar</a>
                <a href="https://github.com/imankgoyal">Github</a>
                <a href="https://www.linkedin.com/in/ankit-goyal-5baaa287/">LinkedIn</a>
                <a href="https://x.com/imankitgoyal">Twitter</a>
             </p>
	     <p>&nbsp;</p>
	   </td>
	  </tr>
        </table>
	<div class = "row">
        <table width="100%" align="center" margin-top=100px class="institution-timeline">
            <tr>
              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/nvidia.png" width="50%">
              </td>

              <td align="center" width="16%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/princeton.png" width="30%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/intel.png" width="60%">
              </td>

              <td align="center" width="16%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/michigan.png" width="50%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/msr.png" width="90%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/usc.png" width="30%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/iitk.png" width="60%">
              </td>
            </tr>

            <tr>
                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Scientist<br>NVIDIA<br>Current</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">PhD, CS<br>Princeton University<br>2018 - 2022</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Intern<br>Intel<br>Winter 2021</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">MS, CSE<br>University of Michigan<br>2016 - 2018</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Intern<br>MSR<br>Summer 2016</td>
		
                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Intern<br>USC<br>Summer 2015</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">BTech, EE<br>IIT Kanpur<br>2012 - 2016</td>
          </tr>

          </table>
      </div>

        <!-- Recent News Section -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td width="100%" valign="middle">
        <heading>Recent News</heading>
      </td>
    </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="top" class="news-section">
              <ul>
                <li><i>Oct 2025:</i> Released <a href="https://vla0.github.io/">VLA-0</a>, a simple yet state-of-the-art approach to building Vision-Language-Action models.</li>
                <li><i>Oct 2025:</i> Gave an invited talk at <a href="https://opendrivelab.com/iccv2025/workshop/">ICCV 2025 Workshop on Advancing Spatial Understanding for Embodied Intelligence</a>.</li>
                <li><i>Oct 2025:</i> Gave a guest lecture at <a href="https://labs.utdallas.edu/irvl/courses/fall-2025-cs-6341-robotics/">UT Dallas CS 6341 Robotics</a> on "Perspectives on Designing Vision-Language-Action Models".</li>
                <li><i>Sep 2025:</i> <a href="https://maniflow-policy.github.io/">ManiFlow</a> was accepted to CoRL 2025.</li>
                <li><i>Jun 2025:</i> <a href="https://cutamp.github.io/">cuTAMP</a> was accepted to RSS 2025 and covered by <a href="https://news.mit.edu/2025/new-system-enables-robots-to-solve-manipulation-problems-seconds-0605">MIT News</a>.</li>
                <li><i>Feb 2025:</i> <a href="https://jasonqsy.github.io/3DMVP/">3D-MVP</a>, 3D pretraining for manipulation was accepted to CVPR 2025.</li>
                <li><i>Jan 2025:</i> <a href="https://hamster-robot.github.io/">HAMSTER</a>, a hierarchical VLA for open-world manipulation was accepted to ICLR 2025.</li>
              </ul>
              <details class="details-example">
                <summary>Older news</summary>
                <ul>
                  <li><i>Sep 2024:</i> Gave a talk at <a href='https://www.youtube.com/watch?v=E_vOQsXWA30/'>MILA Robot Learning Seminar</a></li>
                  <li><i>Aug 2024:</i> <a href="https://actaim2.github.io">ActAIM2</a>, which discovers self-supervised action modes accepted to CoRL 2024.</li>
                  <li><i>May 2024:</i> <a href="https://robotic-view-transformer-2.github.io/">RVT-2</a> accepted to RSS 2024.</li>
                  <li><i>May 2024:</i> Gave a keynote talk at <a href="https://icra-manipulation-skill.github.io/"> Manipulation Skills Workshop in ICRA</a>.</li>
                  <li><i>Oct 2023:</i> Gave a talk at <a href="https://robotics.cs.washington.edu/colloquium/"> UW Robotics Colloquium </a> along with Caelan Garrett and Iretiayo Akinola.</li>
                  <li><i>Aug 2023:</i> Two papers (including one Oral) accepted to CoRL 2023. <a href="https://robotic-view-transformer.github.io/">RVT</a> and <a href="https://github.com/anthonysimeonov/rpdiff">"Shelving, Stacking and Hanging"</a>.</li>
                  <li><i>June 2023:</i> Released <a href="https://robotic-view-transformer.github.io/">Robotic View Transformer</a> for fast and performant 3D manipulation.</li>
                  <li><i>May 2023:</i> Selected to be a part of the <a href='https://sites.google.com/view/rsspioneers2023/participants?authuser=0'>RSS Pioneers Cohort, 2023</a>.</li>
                  <li><i>Apr 2023:</i> Gave a talk at <a href='https://mila-vision-rg.github.io/'>MILA Vision Reading Group</a>. Thanks for the invite!</li>
                  <li><i>Feb 2023:</i> <a href="https://arxiv.org/abs/2209.11302">ProgPrompt</a> accepted to <b>ICRA 2023</b> - LLM+Robotics led by <a href="https://ishikasingh.github.io/">Ishika</a> and <a href="https://www.cs.cornell.edu/~valts/">Valts</a>.</li>
                  <li><i>Oct 2022:</i> <a href='https://github.com/princeton-vl/Coupled-Iterative-Refinement'>6D pose estimation work</a> led by <a href='https://www.lahavlipson.com/'>Lahav Lipson</a> won an award in ECCV BOP challenge 2022.</li>
                  <li><i>Sep 2022:</i> Received the <a href='https://neurips.cc/Conferences/2022/FinancialAssistance'> NeurIPS Scholar Award</a>.</li>
                  <li><i>Sep 2022:</i> Paper on <a href="https://arxiv.org/abs/2110.07641">non-deep networks</a> accepted to <b>NeurIPS 2022</b>.</li>
                  <li><i>Aug 2022:</i> Defended my Ph.D. thesis and started as a Research Scientist at NVIDIA working with <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a>.</li>
                  <li><i>Mar 2022:</i> Two papers accepted to <b>CVPR 2022</b>.</li>
                  <li><i>Aug 2021:</i> Recognized as an <a href="http://iccv2021.thecvf.com/outstanding-reviewers">outstanding reviewer</a> at <b>ICCV 2021</b>.</li>
                  <li><i>July 2021:</i> Selected for <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2021-north-america">Qualcomm Innovation Fellowship</a> with Zachary Teed!</li>
                </ul>
              </details>
            </td>
          </tr>
        </table>

        <!-- Research Interest -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td width="100%" valign="middle">
        <heading>Selected Publications</heading>
      </td>
    </tr>
        </table>

        <!-- Selected Publications-->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    
    <!-- VLA-0 -->
          <tr class="publication-row">
              <td width="35%">
                  <video width="100%" autoplay loop muted>
                      <source src="https://vla0.github.io/Videos/teaser.mp4" type="video/mp4">
                  </video>
              </td>
              <td valign="top" width="70%">
                  <p>
                      <a href="https://vla0.github.io/">
                          <papertitle>VLA-0: Building State-of-the-Art VLAs with Zero Modification</papertitle>
                      </a>
                      <br>
                      <strong>Ankit Goyal</strong>, Hugo Hadfield, Xuning Yang, Valts Blukis, Fabio Ramos
                      <br>
                      <em>arXiv 2025</em> 
                      <br>
                      <a href="https://vla0.github.io/">[project page]</a> 
                      <a href="https://arxiv.org/abs/2510.13054">[paper]</a>
                      <p align="justify">
                        We introduce VLA-0, a surprisingly simple approach to building Vision-Language-Action models that achieves state-of-the-art results by representing actions directly as text, without any architectural changes to the base VLM. VLA-0 outperforms all methods trained on the same robotic data on LIBERO benchmark and even surpasses models with large-scale pretraining.
                      </p>
                  </p>
              </td>
          </tr>

    <!-- HAMSTER -->
          <tr class="publication-row">
              <td width="35%">
                  <video width="100%" autoplay loop muted>
                      <source src="https://hamster-robot.github.io/figs/hamster_teaser_v2.mp4" type="video/mp4">
                  </video>
              </td>
              <td valign="top" width="70%">
                  <p>
                      <a href="https://hamster-robot.github.io/">
                          <papertitle>HAMSTER: Hierarchical Action Models for Open-World Manipulation</papertitle>
                      </a>
                      <br>
                      Yi Li*, Yuquan Deng*, Jesse Zhang*, Joel Jang, Marius Memmel, Raymond Yu, Caelan Reed Garrett, Fabio Ramos, Dieter Fox, Anqi Li^, Abhishek Gupta^, <strong>Ankit Goyal</strong>^
                      <br>
                      <em>ICLR 2025</em> 
                      <br>
                      <a href="https://github.com/liyi14/HAMSTER_beta">[code]</a> 
                      <a href="https://hamster-robot.github.io/">[project page]</a> 
                      <p align="justify">
                        We introduce HAMSTER, a Hierarchical Vision-Language-Action (VLA) architecture designed for robotic manipulation. This approach effectively combines the advantages of imitation learning models, which require little in-domain robot data, with those of large VLA models that can generalize well.
                      </p>
                  </p>
              </td>
          </tr>

    <!-- 3D-MVP -->
          <tr class="publication-row">
              <td width="35%">
                  <img src='https://jasonqsy.github.io/3DMVP/static/images/architecture.png' width=100%>
              </td>
              <td valign="top" width="70%">
                  <p>
                      <a href="https://jasonqsy.github.io/3DMVP/">
                          <papertitle>3D-MVP: 3D Multiview Pretraining for Robotic Manipulation</papertitle>
                      </a>
                      <br>
                      Shengyi Qian, Kaichun Mo, Valts Blukis, David F. Fouhey, Dieter Fox, <strong>Ankit Goyal</strong>
                      <br>
                      <em>CVPR 2025</em> 
                      <br>
                      <!-- <a href="https://github.com/jasonqsy/3DMVP">[code]</a>  -->
                      <a href="https://jasonqsy.github.io/3DMVP/">[project page]</a> 
                      <a href="https://arxiv.org/abs/2406.18158">[paper]</a>
                      <p align="justify">
                        We propose 3D multi-view pretraining using MAEs for robot manipulation. 
                      </p>
                  </p>
              </td>
          </tr>

	  <!-- RVT-2 -->
          <tr class="publication-row">
              <td width="35%">
                  <img src='https://robotic-view-transformer-2.github.io/figs/teaser.gif' width=100%>
              </td>
              <td valign="top" width="70%">
                  <p>
                      <a href="https://robotic-view-transformer-2.github.io/">
                          <papertitle>RVT-2: Learning Precise Manipulation from Few Examples</papertitle>
                      </a>
                      <br>
                      <strong>Ankit Goyal</strong>, Valts Blukis, Jie Xu, Yijie Guo, Yu-Wei Chao, Dieter Fox
                      <br>
                      <em>RSS 2024</em> 
                      <br>
                      <a href="https://github.com/nvlabs/rvt">[code]</a> 
                      <a href="https://robotic-view-transformer-2.github.io/">[project page]</a> 
                      <p align="justify">
                          We study how to build a robotic system that can solve high-precision manipulation tasks from a few demonstrations. Prior works, like PerAct and RVT, have studied few-shot manipulation; however, they often struggle with tasks requiring high precision. We study how to make them more effective, precise, and fast. Using a combination of architectural and system-level improvements, we propose RVT-2, a multitask 3D manipulation model that is 6X faster in training and 2X faster in inference than its predecessor RVT. RVT-2 achieves a new state-of-the-art on RLBench.
                      </p>
                  </p>
              </td>
          </tr>

	  <!-- RVT -->
          <tr class="publication-row">
              <td width="35%">
              <img src='https://robotic-view-transformer.github.io/real_world/real_world_small.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://robotic-view-transformer.github.io/">
                  <papertitle>RVT: Robotic View Transformer for 3D Object Manipulation</papertitle>
                </a>
                <br>
		            <strong>Ankit Goyal</strong>, Jie Xu, Yijie Guo, Valts Blukis, Yu-Wei Chao, Dieter Fox
                <br>
		<em>CoRL 2023 (Oral)</em> 
		<br>
		<a href="https://github.com/nvlabs/rvt">[code]</a> 
		<a href="https://robotic-view-transformer.github.io/">[project page]</a> 
		<a href="https://www.youtube.com/watch?v=mIQN4f3KSA8">[video]</a> 
		<a href="data/rvt.key">[slides]</a> 


		<p align="justify"> RVT is a multi-view transformer for 3D manipulation that is both scalable and accurate.  In simulations, a single RVT model works well across 18 RLBench tasks with 249 task variations, achieving 26% higher relative success than existing  SOTA (PerAct). It also trains 36X faster than PerAct for achieving the same performance and achieves 2.3X the inference speed of PerAct. Further, RVT can perform a variety of manipulation tasks in the real world with just a few (~10) demonstrations per task.</p>
            </td>
          </tr>

	  <!-- RPDiff -->
          <tr class="publication-row">
              <td width="35%">
		    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="https://anthonysimeonov.github.io/rpdiff-multi-modal/img/book_bookshelf_seq1_8x_rdc_lout.mp4" type="video/mp4">
                </video>
              <!-- <iframe src="https://anthonysimeonov.github.io/rpdiff-multi-modal/img/book_bookshelf_seq1_8x_rdc_lout.mp4" -->
              <!--     frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
              <!-- <img src='https://anthonysimeonov.github.io/rpdiff-multi-modal/img/book_bookshelf_seq1_8x_rdc_lout.mp4' width=100%> -->
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://anthonysimeonov.github.io/rpdiff-multi-modal/">
                  <papertitle>Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement</papertitle>
                </a>
                <br>
		Anthony Simeonov, Ankit Goyal, Lucas Manuelli, Lin Yen-Chen, Alina Sarmiento, Alberto Rodriguez, Pulkit Agrawal, Dieter Fox
                <br>
		<em>CoRL 2023</em> 
		<br>
		<a href="https://github.com/anthonysimeonov/rpdiff">[code]</a> 
		<a href="https://anthonysimeonov.github.io/rpdiff-multi-modal/">[project page]</a> 
		<a href="https://www.youtube.com/watch?v=x9noTl_aqu0">[video]</a> 
		<p align="justify"> RPDiff rearranges objects into "multimodal" configurations, such as a book inserted in an open slot of a bookshelf. It generalizes to novel geometries, poses, and layouts, and is trained from demonstrations to operate on point clouds.</p>
            </td>
          </tr>

	  <!-- Infinigen -->
          <tr class="publication-row">
              <td width="35%">
              <img src='https://infinigen.org/img/random_sample.jpeg' width=100%>
              <!-- <iframe src="https://www.youtube.com/embed/6tgspeI-GHY?rel=0&amp;autoplay=1&mute=1&vq=small"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen loading="lazy"></iframe> -->
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://infinigen.org/">
                  <papertitle>Infinite Photorealistic Worlds using Procedural Generation</papertitle>
                </a>
                <br>
		Alexander Raistrick, Lahav Lipson, Zeyu Ma, Lingjie Mei, Mingzhe Wang, Yiming Zuo, Karhan Kayan, Hongyu Wen, Beining Han, Yihan Wang, Alejandro Newell, Hei Law, <strong>Ankit Goyal</strong>, Kaiyu Yang, and Jia Deng
                <br>
		<em>CVPR 2023</em> 
		<br>
		<a href="https://infinigen.org/">[project page]</a> 
		<p align="justify">Infinigen is a generator of unlimited high-quality 3D data. Procedural and open-source.</p>
            </td>
          </tr>

	  <!-- ProgPrompt -->
          <tr class="publication-row">
              <td width="35%">
              <img src='images/progprompt.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2209.11302">
                  <papertitle>ProgPrompt: Generating Situated Robot Task Plans using Large Language Models</papertitle>
                </a>
                <br>
		Ishika Singh, Valts Blukis, Arsalan Mousavian, <strong>Ankit Goyal</strong>, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg 
                <br>
		<em>ICRA 2023</em> 
		<br>
		Also in <em>Autonomous Robots</em>, <em>LaRel @ NeurIPS 2022</em> and <em>LangRob @ CoRL 2022</em>
		<br>
		<a href="https://progprompt.github.io/">[project page]</a> 
		<p align="justify">We use large language models (LLMs) for task planning in robotics. We construct pythonic prompts, which specify the task, robot capabilities and the environment to seed LLMs.</p>
            </td>
          </tr>
	  <!-- IFOR -->

          <tr class="publication-row">
            <td width="35%">
              <img src='images/ifor.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2202.00732">
                  <papertitle>IFOR: Iterative Flow Minimization for Robotic Object Rearrangement</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Arsalan Mousavian, Chris Paxton, Yu-Wei Chao, Brian Okorn, Jia Deng, Dieter Fox
                <br>
		<em>CVPR 2022</em> 
		<br>
		Also in <em>EAI @ CVPR 2022</em>
		<br>
		<a href="https://imankgoyal.github.io/ifor.html">[project page]</a> 
		<a href="data/ifor_slides.pdf">[slides]</a> 
		<a href="data/ifor_poster.pdf">[poster]</a> 
		<p align="justify">IFOR is an end-to-end method for the challenging problem of object rearrangement for unknown objects given an RGBD image of the original and final scenes. It works on cluttered scenes in the real world, while training only on synthetic data. </p>
            </td>
          </tr>

	  <!-- 6DOF -->
          <tr class="publication-row">
            <td width="35%">
              <img src='images/6dof.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2204.12516">
                  <papertitle>Coupled Iterative Refinement for 6D Multi-Object Pose Estimation</papertitle>
                </a>
                <br>
                Lahav Lipson, Zachary Teed, <strong>Ankit Goyal</strong>, Jia Deng
                <br>
                <em>CVPR 2022</em>               
		<br>
		<a href="https://arxiv.org/abs/2204.12516">[paper]</a> <a href="https://github.com/princeton-vl/Coupled-Iterative-Refinement">[code]</a><br> 
		<p align="justify">We propose state-of-the-art 6DOF multi-object pose estimation system. Our system iteratively refines object pose and correspondece.</p>
            </td>
          </tr>

	  <!-- Non-deep Networks -->
          <tr class="publication-row">
              <td width="35%">
              <img src='images/nondeep.jpg' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2110.07641">
                  <papertitle>Non-deep Networks</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Alexey Bochkovskiy, Jia Deng, Vladlen Koltun
                <br>
                <em>NeurIPS 2022</em>                
		<br>
		<a href="https://github.com/imankgoyal/NonDeepNetworks">[code]</a> 
		<a href="data/non_deep_poster.pdf">[poster]</a> 
		<a href="data/non_deep_conf.key">[slides]</a> 
		<a href="https://neurips.cc/virtual/2022/poster/55098">[video]</a> 
		<p align="justify"> Depth is the hallmark of DNNs. But more depth means more sequential computation and higher latency. This begs the question -- is it possible to build high-performing ``non-deep" neural networks? We show it is. </p>
            </td>
          </tr>
	
	  <!-- SimpleView -->
          <tr class="publication-row">
              <td width="35%">
              <img src='images/simpleview.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2106.05304">
                  <papertitle>Revisiting Point Cloud Shape Classification with a Simple and Effective Baseline</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Hei Law, Bowei Liu, Alejnadro Newell, Jia Deng 
                <br>
                <em>ICML</em> 2021                
		<br>
                <a href="https://github.com/princeton-vl/SimpleView">[code]</a> 
		<a href="https://docs.google.com/presentation/d/1eYKDth19S2Ovt1lO0CpbzCM_hh7QEVni/edit?usp=drive_link&ouid=111606148346153000949&rtpof=true&sd=true">[slides]</a>
		<a href="https://drive.google.com/file/d/1TZIbcEaSQmjyOUiURYVzYthduzui1Rcx/view?usp=drive_link">[poster]</a>
		<a href="https://icml.cc/virtual/2021/poster/9099">[video]</a>
                <p align="justify"> Many point-based approaches have been proposed reporting steady benchmark improvements over time. We study the key ingredients of this progress and uncover two critical results. First, auxiliary factors, independent of the model architecture, make a large difference in performance. Second, a very simple projection based method performs surprisingly well. </p>
            </td>
          </tr>
          
          <!-- Rel3D -->
          <tr class="publication-row">
              <td width="35%">
              <img src='images/rel3D.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://papers.nips.cc/paper/2020/hash/76dc611d6ebaafc66cc0879c71b5db5c-Abstract.html">
                  <papertitle>Rel3D: A Minimally Contrastive Benchmark for Grounding Spatial Relations in 3D</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Kaiyu Yang, Dawei Yang, <a href = "http://www.cs.princeton.edu/~jiadeng/"> Jia Deng</a>
                <br>
                <em>NeuRIPS</em> 2020, <span style="color:brown;">Spotlight (Top 4% of submitted papers)</span>                
		<br>
                <a href="https://github.com/princeton-vl/Rel3D">[code]</a> 
		<a href="https://drive.google.com/file/d/1lWA2WlJR4itJJrnHMRiZ87-z8akagkH4/view?usp=sharing">[slides]</a>
		<a href="data/rel3d_poster.pdf">[poster]</a>
		<a href="https://slideslive.com/38935853/rel3d-a-minimally-contrastive-benchmark-or-grounding-spatial-relations-in-3d">[video]</a>
                <p align="justify"> Understanding spatial relations is important for both humans and robots. We create Rel3D, the first large-scale, human-annotated dataset for grounding spatial relations in 3D.  The 3D scenes in Rel3D come in minimally contrastive pairs: two scenes in a pair are almost identical, but a spatial relation holds in one and fails in the other. </p>
            </td>
          </tr>

          <!-- PackIt -->
          <tr class="publication-row">
              <td width="35%">
              <img src='images/packit.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2007.11121">
                  <papertitle>PackIt: A Virtual Environment for Geometric Planning</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, <a href = "http://www.cs.princeton.edu/~jiadeng/"> Jia Deng</a>
                <br>
                <em>ICML</em> 2020
                <br>
                <a href="https://github.com/princeton-vl/PackIt">[code]</a>
                <a href="data/packit_slides.pptx">[slides]</a>
		<a href="https://slideslive.com/38927501/packit-a-virtual-environment-for-geometric-planning?ref=search">[video]</a>
                <p align="justify"> Simultaneously reasoning about geometry and planning action is crucial for intelligent agents. This ability of geometric planning comes in handy while grocery shopping, rearranging room, warehouse management etc. We create PackIt, a virtual environment that caters to geometric planning.</p>
            </td>
          </tr>

          <!-- Think Visually -->
          <tr class="publication-row">
              <td width="35%">
              <img src='images/dsmn2.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/1805.11025">
                  <papertitle>Think Visually: Question Answering through Virtual Imagery</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Jian Wang, <a href = "http://www.cs.princeton.edu/~jiadeng/"> Jia Deng</a>
                <br>
                <em>ACL</em> 2018
                <br>
                <a href="https://github.com/princeton-vl/think_visually">[code]</a>
                <a href="data/think_visually_poster.pdf">[poster]</a>
                <p align="justify">We study geometric reasoning in the context of question-answering. We introduce Dynamic Spatial Memory Network (DSMN), a deep network architecture designed for answering questions that admit latent visual representations. </p>
            </td>
          </tr>

          <!-- ProtoNN -->
          <tr class="publication-row">
              <td width="35%">
              <img src='images/protonn.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <a href="http://proceedings.mlr.press/v70/gupta17a.html">
                <papertitle>ProtoNN: Compressed and Accurate kNN for Resource-scarce Devices</papertitle>
              </a>
              <br>
              C Gupta, AS Suggala,<strong> A Goyal</strong>, HV Simhadri, BP, AK, SG, RU, MV, <a href="https://www.microsoft.com/en-us/research/people/prajain/">P Jain</a>
              <br>
              <em>ICML</em> 2017
              <br>
              <a href="https://github.com/Microsoft/EdgeML">[code]</a>
              <br><br>
              <a href="https://patents.google.com/patent/US20180330275A1/en">
                <papertitle>Resource-Efficient Machine Learning</papertitle>
              </a>
              <br>
              Prateek Jain, Chirag Gupta, AS Suggala,<strong> Ankit Goyal</strong>, HV Simhadri
              <br>
              <em>US Patent Applicaiton</em>
              <br>
              <p align="justify">We propose ProtoNN, a novel algorithm that addresses the problem of real-time and accurate prediction on resource-scarce devices. </p>
             
            </td>
          </tr>

          <!-- Emotion Prediction -->
          <tr class="publication-row">
              <td width="35%">
              <img src='images/multimodal.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <a href="https://ieeexplore.ieee.org/abstract/document/7472192/">
                <papertitle>A Multimodal Mixture-Of-Experts Model for Dynamic Emotion Prediction in Movies</papertitle>
              </a>
              <br>
              <strong>Ankit Goyal</strong>, <a href="https://algoseer.github.io/">Naveen Kumar</a>, <a href="http://tanayag.com/Home.html">Tanaya Guha</a>, <a href="https://sail.usc.edu/people/shri.php">Shrikanth S. Narayanan</a>
              <br>
              <em>ICASSP</em> 2016
              <br>
              <p align="justify">We address the problem of continuous emotion prediction in movies. We propose a Mixture of Experts (MoE)-based fusion model that dynamically combines information from the audio and video modalities for predicting the emotion evoked in movies. </p>
              
            </td>
          </tr>

          <!-- <!-1- SURF -1-> -->
          <!-- <tr> -->
          <!--   <td width="35%"> -->
          <!--     <img src='images/surf.png' width=100%> -->
          <!--   </td> -->
          <!--   <td valign="top" width="70%"> -->
          <!--     <a href="https://link.springer.com/chapter/10.1007/978-3-319-27000-5_34"> -->
          <!--       <papertitle>Object Matching Using Speeded Up Robust Features</papertitle> -->
          <!--     </a> -->
          <!--     <br> -->
          <!--     NK Verma, <strong> Ankit Goyal</strong>, A Harsha Vardhan, Rahul Kumar Sevakula, Al Salour -->
          <!--     <br> -->
          <!--     <em>IES</em> 2016 -->
          <!--     <br> -->
          <!--     <p align="justify"> We propose a robust algorithm which is capable of detecting all the instances of a particular object in a scene image using Speeded Up Robust Features. </p> -->
          <!--   </td> -->
          <!-- </tr> -->

          <!-- <!-1- Template Matching -1-> -->
          <!-- <tr> -->
          <!--   <td width="35%"> -->
          <!--     <img src='images/template.png' width=100%> -->
          <!--   </td> -->
          <!--   <td valign="top" width="70%"> -->
          <!--     <p> -->
          <!--       <a href="https://ieeexplore.ieee.org/abstract/document/7334132/"> -->
          <!--         <papertitle>Template Matching for Inventory Management using Fuzzy Color Histogram and Spatial Filters</papertitle> -->
          <!--       </a> -->
          <!--       <br> -->
          <!--       NK Verma,<strong> Ankit Goyal</strong>, Anadi Chaman, Rahul K Sevakula, Al Salour -->
          <!--       <br> -->
          <!--       <em>ICIEA</em> 2015 -->
          <!--       <br> -->
          <!--       <p align="justify">We propose a methodology for object counting using color histogram based segmentation and spatial filters. </p> -->
          <!--    </p> --> 
          <!--   </td> -->
          <!-- </tr> -->

        </table>

        <!-- Media Coverage -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td width="100%" valign="middle">
        <heading>Media Coverage</heading>
      </td>
    </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <!-- Selected Coverage -->
          <!-- CVPR News - 3D-MVP -->
          <tr class="publication-row">
            <td width="15%" align="center" valign="top">
              <a href="https://cvpr.thecvf.com/Conferences/2025/News/AI_Enhanced_Robotics">
                <img src='https://www.nvidia.com/content/nvidiaGDC/us/en_US/events/cvpr/_jcr_content/root/responsivegrid/nv_container_copy_co/nv_container/nv_image.coreimg.100.1290.png/1749620067706/cvpr-2025-logo.png' style="width:100%; max-width:120px; object-fit:contain;">
              </a>
            </td>
            <td width="85%" valign="top">
              <p>
                <a href="https://cvpr.thecvf.com/Conferences/2025/News/AI_Enhanced_Robotics">
                  <strong>CVPR News</strong>
                </a>
                <br>
                <i>Work covered:</i> <a href="https://jasonqsy.github.io/3DMVP/">3D-MVP</a>
                <br>
                Featured in CVPR 2025 AI-Enhanced Robotics highlights.
              </p>
            </td>
          </tr>

          <!-- NVIDIA Blog - HAMSTER -->
          <tr class="publication-row">
            <td width="15%" align="center" valign="top">
              <a href="https://blogs.nvidia.com/blog/ai-research-iclr-2025/">
                <img src='https://s3.amazonaws.com/cms.ipressroom.com/219/files/20237/64e3dc1a3d6332319b2dfd35_NVIDIA-logo-white-16x9/NVIDIA-logo-white-16x9_927581a6-fc31-4fa5-85c6-379680c6aa6c-prv.png' style="width:100%; max-width:120px; object-fit:cover;">
              </a>
            </td>
            <td width="85%" valign="top">
              <p>
                <a href="https://blogs.nvidia.com/blog/ai-research-iclr-2025/">
                  <strong>NVIDIA Blog</strong>
                </a>
                <br>
                <i>Work covered:</i> <a href="https://hamster-robot.github.io/">HAMSTER</a>
                <br>
                NVIDIA highlights HAMSTER research at ICLR 2025.
              </p>
            </td>
          </tr>

          <!-- MIT News - cuTAMP -->
          <tr class="publication-row">
            <td width="15%" align="center" valign="top">
              <a href="https://news.mit.edu/2025/new-system-enables-robots-to-solve-manipulation-problems-seconds-0605">
                <img src='https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/MIT_News_logo.png/500px-MIT_News_logo.png' style="width:100%; max-width:120px; object-fit:contain;">
              </a>
            </td>
            <td width="85%" valign="top">
              <p>
                <a href="https://news.mit.edu/2025/new-system-enables-robots-to-solve-manipulation-problems-seconds-0605">
                  <strong>MIT News</strong>
                </a>
                <br>
                <i>Work covered:</i> <a href="https://cutamp.github.io/">cuTAMP</a>
                <br>
                New system enables robots to solve manipulation problems in seconds.
              </p>
            </td>
          </tr>

          <!-- The Robot Report - RVT -->
          <tr class="publication-row">
            <td width="15%" align="center" valign="top">
              <a href="https://www.therobotreport.com/nvidias-rvt-can-learn-new-tasks-after-just-10-demos/">
                <img src='https://www.therobotreport.com/wp-content/uploads/2018/01/the-robot-report-business-of-robots-Lisa-Eitel-Dan-Kara-Steve-Crowe.jpg' style="width:100%; max-width:120px; object-fit:cover;">
              </a>
            </td>
            <td width="85%" valign="top">
              <p>
                <a href="https://www.therobotreport.com/nvidias-rvt-can-learn-new-tasks-after-just-10-demos/">
                  <strong>The Robot Report</strong>
                </a>
                <br>
                <i>Work covered:</i> <a href="https://robotic-view-transformer.github.io/">RVT</a>
                <br>
                NVIDIA's RVT Can Learn New Tasks After Just 10 Demos.
              </p>
            </td>
          </tr>

          <!-- Hacker News - Infinigen -->
          <tr class="publication-row">
            <td width="15%" align="center" valign="top">
              <a href="https://news.ycombinator.com/item?id=36376071">
                <img src='https://news.ycombinator.com/y18.svg' style="width:100%; max-width:120px;">
              </a>
            </td>
            <td width="85%" valign="top">
              <p>
                <a href="https://news.ycombinator.com/item?id=36376071">
                  <strong>Hacker News</strong>
                </a>
                <br>
                <i>Work covered:</i> <a href="https://infinigen.org/">Infinigen</a>
                <br>
                Featured on Hacker News front page.
              </p>
            </td>
          </tr>

          <!-- Collapsible Other Coverage -->
          <tr>
            <td colspan="2">
              <details class="details-example">
                <summary>Other coverage</summary>
                <table width="100%" border="0" cellspacing="0" cellpadding="10">
                  <!-- RVT-2 - Hackster.io -->
                  <tr class="publication-row">
                    <td width="15%" align="center" valign="top">
                      <a href="https://www.hackster.io/news/rvt-2-is-a-fast-learner-0469d3578acf">
                        <img src='https://www.hackster.io/assets/hackster_logo_squared.png' style="width:100%; max-width:120px;">
                      </a>
                    </td>
                    <td width="85%" valign="top">
                      <p>
                        <a href="https://www.hackster.io/news/rvt-2-is-a-fast-learner-0469d3578acf">
                          <strong>Hackster.io</strong>
                        </a>
                        <br>
                        <i>Work covered:</i> <a href="https://robotic-view-transformer-2.github.io/">RVT-2</a>
                        <br>
                        RVT-2 is a Fast Learner - Coverage of our robotic manipulation research.
                      </p>
                    </td>
                  </tr>

                  <!-- RVT-2 - Medium -->
                  <tr class="publication-row">
                    <td width="15%" align="center" valign="top">
                      <a href="https://medium.com/ai-disruption/nvidia-releases-general-robot-model-rvt-2-with-6x-training-efficiency-boost-55b61a1e558b">
                        <img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5NKHQDjC1cUC441HTejxuQ.png" style="width:100%; max-width:120px; object-fit:cover;">
                      </a>
                    </td>
                    <td width="85%" valign="top">
                      <p>
                        <a href="https://medium.com/ai-disruption/nvidia-releases-general-robot-model-rvt-2-with-6x-training-efficiency-boost-55b61a1e558b">
                          <strong>Medium - AI Disruption</strong>
                        </a>
                        <br>
                        <i>Work covered:</i> <a href="https://robotic-view-transformer-2.github.io/">RVT-2</a>
                        <br>
                        NVIDIA Releases General Robot Model RVT-2 with 6X Training Efficiency Boost.
                      </p>
                    </td>
                  </tr>

                  <!-- RVT-2 - Moomoo -->
                  <tr class="publication-row">
                    <td width="15%" align="center" valign="top">
                      <a href="https://www.moomoo.com/news/post/47625914/nvidia-releases-the-general-robot-model-rvt-2-accelerating-the?level=1&data_ticket=1751133579898100">
                        <img src='https://cdn.futustatic.com/moomoo_node/assets/images/moo.19ace43571.png' style="width:100%; max-width:120px; object-fit:cover;">
                      </a>
                    </td>
                    <td width="85%" valign="top">
                      <p>
                        <a href="https://www.moomoo.com/news/post/47625914/nvidia-releases-the-general-robot-model-rvt-2-accelerating-the?level=1&data_ticket=1751133579898100">
                          <strong>Moomoo News</strong>
                        </a>
                        <br>
                        <i>Work covered:</i> <a href="https://robotic-view-transformer-2.github.io/">RVT-2</a>
                        <br>
                        NVIDIA Releases the General Robot Model RVT-2, Accelerating the Robotics Revolution.
                      </p>
                    </td>
                  </tr>

                  <!-- RVT - Hackster.io -->
                  <tr class="publication-row">
                    <td width="15%" align="center" valign="top">
                      <a href="https://www.hackster.io/news/a-new-perspective-on-3d-object-manipulation-7d912844bc73">
                        <img src='https://www.hackster.io/assets/hackster_logo_squared.png' style="width:100%; max-width:120px;">
                      </a>
                    </td>
                    <td width="85%" valign="top">
                      <p>
                        <a href="https://www.hackster.io/news/a-new-perspective-on-3d-object-manipulation-7d912844bc73">
                          <strong>Hackster.io</strong>
                        </a>
                        <br>
                        <i>Work covered:</i> <a href="https://robotic-view-transformer.github.io/">RVT</a>
                        <br>
                        A New Perspective on 3D Object Manipulation.
                      </p>
                    </td>
                  </tr>

                  <!-- RVT - MarkTechPost -->
                  <tr class="publication-row">
                    <td width="15%" align="center" valign="top">
                      <a href="https://marktechpost-newsletter.beehiiv.com/p/ai-news-baidu-challenges-gpt-theorem-proving-llms-motiongpt-ai-updates-june-28-2023-edition">
                        <img src='https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/publication/logo/f5e63dd4-5653-4f09-83e2-321a8b1ba526/thumb_AI_PAPERS_SUMMARIES.png' style="width:100%; max-width:120px;">
                      </a>
                    </td>
                    <td width="85%" valign="top">
                      <p>
                        <a href="https://marktechpost-newsletter.beehiiv.com/p/ai-news-baidu-challenges-gpt-theorem-proving-llms-motiongpt-ai-updates-june-28-2023-edition">
                          <strong>MarkTechPost Newsletter</strong>
                        </a>
                        <br>
                        <i>Work covered:</i> <a href="https://robotic-view-transformer.github.io/">RVT</a>
                        <br>
                        AI News: Featured in weekly AI updates (June 28, 2023 Edition).
                      </p>
                    </td>
                  </tr>

                  <!-- Non-Deep - TDS -->
                  <tr class="publication-row">
                    <td width="15%" align="center" valign="top">
                      <a href="https://towardsdatascience.com/non-deep-networks-b0b80c65c7c6/">
                        <img src='https://miro.medium.com/v2/resize:fit:1200/1*sHhtYhaCe2Uc3IU0IgKwIQ.png' style="width:100%; max-width:120px; object-fit:contain;">
                      </a>
                    </td>
                    <td width="85%" valign="top">
                      <p>
                        <a href="https://towardsdatascience.com/non-deep-networks-b0b80c65c7c6/">
                          <strong>Towards Data Science</strong>
                        </a>
                        <br>
                        <i>Work covered:</i> <a href="https://arxiv.org/abs/2110.07641">Non-Deep Networks</a>
                        <br>
                        In-depth paper review: "Less is the new more?" - ParNet matching performance with just 12 layers.
                      </p>
                    </td>
                  </tr>

                  <!-- Non-Deep - Medium -->
                  <tr class="publication-row">
                    <td width="15%" align="center" valign="top">
                      <a href="https://sh-tsang.medium.com/brief-review-parnet-non-deep-networks-e2e1d0d243aa">
                        <img src='https://miro.medium.com/v2/resize:fit:1200/1*sHhtYhaCe2Uc3IU0IgKwIQ.png' style="width:100%; max-width:120px; object-fit:contain;">
                      </a>
                    </td>
                    <td width="85%" valign="top">
                      <p>
                        <a href="https://sh-tsang.medium.com/brief-review-parnet-non-deep-networks-e2e1d0d243aa">
                          <strong>Medium - Sik-Ho Tsang</strong>
                        </a>
                        <br>
                        <i>Work covered:</i> <a href="https://arxiv.org/abs/2110.07641">Non-Deep Networks</a>
                        <br>
                        Brief Review: Parallel subnetworks instead of stacking one layer after another.
                      </p>
                    </td>
                  </tr>

                  <!-- Non-Deep - Synced -->
                  <tr class="publication-row">
                    <td width="15%" align="center" valign="top">
                      <a href="https://syncedreview.com/2021/10/22/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-129/">
                        <img src='images/synced.png' style="width:100%; max-width:120px;">
                      </a>
                    </td>
                    <td width="85%" valign="top">
                      <p>
                        <a href="https://syncedreview.com/2021/10/22/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-129/">
                          <strong>Synced Review</strong>
                        </a>
                        <br>
                        <i>Work covered:</i> <a href="https://arxiv.org/abs/2110.07641">Non-Deep Networks</a>
                        <br>
                        Featured in article about high-performance ML frameworks and architectures.
                      </p>
                    </td>
                  </tr>
                </table>
              </details>
            </td>
          </tr>

        </table>

        <!-- Teaching -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td width="100%" valign="middle">
                <heading>Teaching</heading>
                <p>
                  I have been a Teaching Assistant for the following courses:
                    <ul>
                      <li> COS529: Advanced Computer Vision at Princeton University [Winter 2020] </li>
                      <li> COS429: Computer Vision at Princeton University [Fall 2018] </li>
                      <li> EECS442: Computer Vision at University of Michigan  [Fall 2017, Winter 2018]</li>
                    </ul>
                </p>
<!-- 		Some <a href="https://imankgoyal.github.io/related.html">related papers</a> to mine. -->
              </td>
            </tr>
        </table>

        <!-- Reference -->
        <p align="right"><a href="https://jonbarron.info/">[Web Cite]</a></p>
      </td>
    </tr>
  </table>  

</body>

</html>
