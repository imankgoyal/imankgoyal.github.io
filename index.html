<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-129673183-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-129673183-1');
</script>

  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
    
    /* Publication row hover effect */
    .publication-row {
      transition: all 0.3s ease;
      border-radius: 8px;
      padding: 10px;
    }
    
    .publication-row:hover {
      transform: translateY(-3px);
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
      background-color: rgba(23, 114, 208, 0.02);
    }
  </style>
  <link rel="icon" type="image/png" href="images/icon.png">
  <title>Ankit Goyal</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <!-- Bio and Image -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="80%" valign="middle">
              <p align="center">
                <name>Ankit Goyal</name>
              </p>
	      <p>&nbsp;</p>
              <p align="justify">
		I am a Research Scientist in Robotics at NVIDIA. I did my Ph.D. in Computer Science at Princeton University, where I was advised by <a href = "http://www.cs.princeton.edu/~jiadeng/">Prof. Jia Deng</a>. I completed Masters from University of Michigan and Bachelors from IIT Kanpur. 
	      <p>
	      <!-- <p align="justify" style="color:red;"> -->
	      <!--   <b><span color="red"> --> 
		<!-- I am looking for full-time research positions starting from Summer/Fall 2022. -->
		<!-- </span></b> -->
	      <!-- </p> -->
	      <p align="justify"> 
	      	I have been fortunate to intern at some wonderful places and work with amazing mentors.
	        <ul>
	          <li> <i>Summer 2021:</i> Nvidia with <a href="https://homes.cs.washington.edu/~fox/"> Dieter Fox </a>
	          <li> <i>Winter 2021:</i> Intel with <a href="http://vladlen.info/"> Vladlen Koltun </a>
	          <li> <i>Summer 2016:</i> Microsoft Research India with <a href="https://www.prateekjain.org/">Prateek Jain</a>
	          <li> <i>Summer 2015:</i> USC with <a href="https://sail.usc.edu/people/shri.php">Shrikanth Narayanan</a>, <a href="http://tanayag.com/Home.html">Tanaya Guha</a> and <a herf="https://navkr.com/"> Naveen Kumar </a>
	        </ul>
              </p>
	      <p align="justify"> <strong> 
	        Recent News: </strong>
	        <ul>
		  <li> <i>Sep 2025:</i> <a href="https://maniflow-policy.github.io/">ManiFlow</a> was accepted to CoRL 2025.
		  <li> <i>Jun 2025:</i> <a href="https://cutamp.github.io/">cuTAMP</a> was accepted to RSS 2025 and covered by <a href="https://news.mit.edu/2025/new-system-enables-robots-to-solve-manipulation-problems-seconds-0605">MIT News</a>.
		  <li> <i>Feb 2025:</i> <a href="https://jasonqsy.github.io/3DMVP/">3D-MVP</a>, 3D pretraining for manipulation was accepted to CVPR 2025.
      <li> <i>Jan 2025:</i> <a href="https://hamster-robot.github.io/">HAMSTER</a>, a hierarchical VLA for open-world manipulation was accepted to ICLR 2025.
      <li> <i>Sep 2024:</i> Gave a talk at <a href='https://www.youtube.com/watch?v=E_vOQsXWA30/'>MILA Robot Learning Seminar</a>
      <li> <i>Aug 2024:</i> <a href="https://actaim2.github.io">ActAIM2</a>, which discovers self-super action modes accepted to CoRL 2024.
		  <li> <i>May 2024:</i> <a href="https://robotic-view-transformer-2.github.io/">RVT-2</a> accepted to RSS 2024.
		  <li> <i>May 2024:</i> Gave a keynote talk at <a href="https://icra-manipulation-skill.github.io/"> Manipulation Skills Workshop in ICRA</a>.
		  <li> <i>Oct 2023:</i> Gave a talk at <a href="https://robotics.cs.washington.edu/colloquium/"> UW Robotics Colloqium </a> along with Caelan Garrett and Iretiayo Akinola.
		  <li> <i>Aug 2023:</i> Two papers (including one Oral) accepted to CoRL 2023. <a href="https://robotic-view-transformer.github.io/">RVT</a> and <a href="https://github.com/anthonysimeonov/rpdiff">"Shelving, Stacking and Hanging"</a>.
		  <li> <i>June 2023:</i> Released <a href="https://robotic-view-transformer.github.io/">Robotic View Transformer</a> for fast and performant 3D manipulation.
		  <li> <i>May 2023:</i> Selected to be a part of the <a href='https://sites.google.com/view/rsspioneers2023/participants?authuser=0'>RSS Pioneers Cohort, 2023</a>.
	    <li> <i>Apr 2023:</i> Gave a talk at <a href='https://mila-vision-rg.github.io/'>MILA Vision Reading Group</a>. Thanks for the invite!
		  <details class="details-example">
    		  <summary>Older news</summary>
		  <ul>
	    <li> <i>Feb 2023:</i> <a href="https://arxiv.org/abs/2209.11302">ProgPrompt</a> accepted to <b>ICRA 2023</b> - LLM+Robotics led by <a herf="https://ishikasingh.github.io/">Ishika</a> and <a herf="https://www.cs.cornell.edu/~valts/">Valts</a>.
			<li> <i>Oct 2022:</i> <a href='https://github.com/princeton-vl/Coupled-Iterative-Refinement'>6D pose estimation work</a> led by <a href='https://www.lahavlipson.com/'>Lahav Lipson</a> won an award in ECCV BOP challenge 2022</b>.
			<li> <i>Sep 2022:</i> Received the <a href='https://neurips.cc/Conferences/2022/FinancialAssistance'> NeurIPS Scholar Award</a></b>.
			<li> <i>Sep 2022:</i> Paper on <a href="https://arxiv.org/abs/2110.07641">non-deep networks</a> accepted to <b>NeurIPS 2022</b>.
			<li> <i>Aug 2022:</i> Defended my Ph.D. thesis and started as a Research Scientist at NVIDIA working with <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a>.	
		  	<li> <i>Mar 2022:</i> Two papers accepted to <b>CVPR 2022</b>.
	          	<li> <i>Aug 2021:</i> Recognized as an <a href="http://iccv2021.thecvf.com/outstanding-reviewers">outstanding reviewer</a> at <b>ICCV 2021</b>.
		  	<li> <i>July 2021:</i> Selected for <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2021-north-america">Qualcomm Innovation Fellowship</a> with Zachary Teed!
		  </ul>
		</details>

	        </ul>
              </p>
	    </td>
            <td width="20%">
              <img src="images/self.png">
            </td>
          </tr>
	  <tr>
            <td colspan="2">
              <p align=center margin=0px>
                <a href="mailto:ankgoyal@umich.edu">Email</a> &nbsp/&nbsp
                <a href="data/AnkitGoyal_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=RhN6jKIAAAAJ&hl=en&oi=sra">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/imankgoyal">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/ankit-goyal-5baaa287/"> LinkedIn </a> &nbsp/&nbsp
	        <a href="https://twitter.com/imankitgoyal?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @imankitgoyal</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
             </p>
	     <p>&nbsp;</p>
	   </td>
	  </tr>
        </table>
	<div class = "row">
        <table width="100%" align="center" margin-top=100px>
            <tr>
              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/nvidia.png" width="50%">
              </td>

              <td align="center" width="16%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/princeton.png" width="30%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/intel.png" width="60%">
              </td>

              <td align="center" width="16%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/michigan.png" width="50%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/msr.png" width="90%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/usc.png" width="30%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/iitk.png" width="60%">
              </td>
            </tr>

            <tr>
                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Scientist<br>NVIDIA<br>Current</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">PhD, CS<br>Princeton University<br>2018 - 2022</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Intern<br>Intel<br>Winter 2021</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">MS, CSE<br>University of Michigan<br>2016 - 2018</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Intern<br>MSR<br>Summer 2016</td>
		
                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Intern<br>USC<br>Summer 2015</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">BTech, EE<br>IIT Kanpur<br>2012 - 2016</td>
          </tr>

          </table>
      </div>

        <!-- Research Interest -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td width="100%" valign="middle">
        <heading>Selected Publications</heading>
        <p align="justify">
          I am interested in understanding various aspects of intelligence, especially reasoning and common sense. In particular, I want to develop computation models for various reasoning skills that humans possess.
        </p>
      </td>
    </tr>
        </table>

        <!-- Selected Publications-->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    
    <!-- VLA-0 -->
          <tr class="publication-row">
              <td width="35%">
                  <video width="100%" autoplay loop muted>
                      <source src="https://vla0.github.io/Videos/teaser.mp4" type="video/mp4">
                  </video>
              </td>
              <td valign="top" width="70%">
                  <p>
                      <a href="https://vla0.github.io/">
                          <papertitle>VLA-0: Building State-of-the-Art VLAs with Zero Modification</papertitle>
                      </a>
                      <br>
                      <strong>Ankit Goyal</strong>, Hugo Hadfield, Xuning Yang, Valts Blukis, Fabio Ramos
                      <br>
                      <em>arXiv 2025</em> 
                      <br>
                      <a href="https://vla0.github.io/">[project page]</a> 
                      <a href="https://arxiv.org/abs/2510.13054">[paper]</a>
                      <p align="justify">
                        We introduce VLA-0, a surprisingly simple approach to building Vision-Language-Action models that achieves state-of-the-art results by representing actions directly as text, without any architectural changes to the base VLM. VLA-0 outperforms all methods trained on the same robotic data on LIBERO benchmark and even surpasses models with large-scale pretraining.
                      </p>
                  </p>
              </td>
          </tr>

    <!-- HAMSTER -->
          <tr class="publication-row">
              <td width="35%">
                  <video width="100%" autoplay loop muted>
                      <source src="https://hamster-robot.github.io/figs/hamster_teaser_v2.mp4" type="video/mp4">
                  </video>
              </td>
              <td valign="top" width="70%">
                  <p>
                      <a href="https://hamster-robot.github.io/">
                          <papertitle>HAMSTER: Hierarchical Action Models for Open-World Manipulation</papertitle>
                      </a>
                      <br>
                      Yi Li*, Yuquan Deng*, Jesse Zhang*, Joel Jang, Marius Memmel, Raymond Yu, Caelan Reed Garrett, Fabio Ramos, Dieter Fox, Anqi Li^, Abhishek Gupta^, <strong>Ankit Goyal</strong>^
                      <br>
                      <em>ICLR 2025</em> 
                      <br>
                      <a href="https://github.com/liyi14/HAMSTER_beta">[code]</a> 
                      <a href="https://hamster-robot.github.io/">[project page]</a> 
                      <br> <br>
                      <a href="https://blogs.nvidia.com/blog/ai-research-iclr-2025/"><img src='https://s3.amazonaws.com/cms.ipressroom.com/219/files/20237/64e3dc1a3d6332319b2dfd35_NVIDIA-logo-white-16x9/NVIDIA-logo-white-16x9_927581a6-fc31-4fa5-85c6-379680c6aa6c-prv.png' style="height:45px; object-fit:cover; width:45px;"></a>
                      <p align="justify">
                        We introduce HAMSTER, a Hierarchical Vision-Language-Action (VLA) architecture designed for robotic manipulation. This approach effectively combines the advantages of imitation learning models, which require little in-domain robot data, with those of large VLA models that can generalize well.
                      </p>
                  </p>
              </td>
          </tr>

    <!-- 3D-MVP -->
          <tr>
              <td width="35%">
                  <img src='https://jasonqsy.github.io/3DMVP/static/images/architecture.png' width=100%>
              </td>
              <td valign="top" width="70%">
                  <p>
                      <a href="https://jasonqsy.github.io/3DMVP/">
                          <papertitle>3D-MVP: 3D Multiview Pretraining for Robotic Manipulation</papertitle>
                      </a>
                      <br>
                      Shengyi Qian, Kaichun Mo, Valts Blukis, David F. Fouhey, Dieter Fox, <strong>Ankit Goyal</strong>
                      <br>
                      <em>CVPR 2025</em> 
                      <br>
                      <!-- <a href="https://github.com/jasonqsy/3DMVP">[code]</a>  -->
                      <a href="https://jasonqsy.github.io/3DMVP/">[project page]</a> 
                      <a href="https://arxiv.org/abs/2406.18158">[paper]</a>
                      <br> <br>
                      <a href="https://cvpr.thecvf.com/Conferences/2025/News/AI_Enhanced_Robotics"><img src='https://www.nvidia.com/content/nvidiaGDC/us/en_US/events/cvpr/_jcr_content/root/responsivegrid/nv_container_copy_co/nv_container/nv_image.coreimg.100.1290.png/1749620067706/cvpr-2025-logo.png' height=40px></a>
                      <p align="justify">
                        We propose 3D multi-view pretraining using MAEs for robot manipulation. 
                      </p>
                  </p>
              </td>
          </tr>

	  <!-- RVT-2 -->
          <tr class="publication-row">
              <td width="35%">
                  <img src='https://robotic-view-transformer-2.github.io/figs/teaser.gif' width=100%>
              </td>
              <td valign="top" width="70%">
                  <p>
                      <a href="https://robotic-view-transformer-2.github.io/">
                          <papertitle>RVT-2: Learning Precise Manipulation from Few Examples</papertitle>
                      </a>
                      <br>
                      <strong>Ankit Goyal</strong>, Valts Blukis, Jie Xu, Yijie Guo, Yu-Wei Chao, Dieter Fox
                      <br>
                      <em>RSS 2024</em> 
                      <br>
                      <a href="https://github.com/nvlabs/rvt">[code]</a> 
                      <a href="https://robotic-view-transformer-2.github.io/">[project page]</a> 
                      <br> <br>
                      <a href="https://www.hackster.io/news/rvt-2-is-a-fast-learner-0469d3578acf">
                          <img src='https://www.hackster.io/assets/hackster_logo_squared.png' height=45px>
                      </a>&nbsp;&nbsp;
                      <a href="https://medium.com/ai-disruption/nvidia-releases-general-robot-model-rvt-2-with-6x-training-efficiency-boost-55b61a1e558b">
                          <img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5NKHQDjC1cUC441HTejxuQ.png" style="width:60px; height:45px; object-fit:cover;">
                      </a>&nbsp;&nbsp;
                      <a href="https://www.moomoo.com/news/post/47625914/nvidia-releases-the-general-robot-model-rvt-2-accelerating-the?level=1&data_ticket=1751133579898100">
                        <img src='https://cdn.futustatic.com/moomoo_node/assets/images/moo.19ace43571.png' style="width:60px; height:45px; object-fit:cover;">
                      </a>
                      
                      <p align="justify">
                          We study how to build a robotic system that can solve high-precision manipulation tasks from a few demonstrations. Prior works, like PerAct and RVT, have studied few-shot manipulation; however, they often struggle with tasks requiring high precision. We study how to make them more effective, precise, and fast. Using a combination of architectural and system-level improvements, we propose RVT-2, a multitask 3D manipulation model that is 6X faster in training and 2X faster in inference than its predecessor RVT. RVT-2 achieves a new state-of-the-art on RLBench.
                      </p>
                  </p>
              </td>
          </tr>

	  <!-- RVT -->
          <tr class="publication-row">
              <td width="35%">
              <img src='https://robotic-view-transformer.github.io/real_world/real_world_small.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://robotic-view-transformer.github.io/">
                  <papertitle>RVT: Robotic View Transformer for 3D Object Manipulation</papertitle>
                </a>
                <br>
		            <strong>Ankit Goyal</strong>, Jie Xu, Yijie Guo, Valts Blukis, Yu-Wei Chao, Dieter Fox
                <br>
		<em>CoRL 2023 (Oral)</em> 
		<br>
		<a href="https://github.com/nvlabs/rvt">[code]</a> 
		<a href="https://robotic-view-transformer.github.io/">[project page]</a> 
		<a href="https://www.youtube.com/watch?v=mIQN4f3KSA8">[video]</a> 
		<a href="data/rvt.key">[slides]</a> 
		<br> <br>
                <a href="https://www.hackster.io/news/a-new-perspective-on-3d-object-manipulation-7d912844bc73"><img src='https://www.hackster.io/assets/hackster_logo_squared.png' height=45px></a>
                <a href="https://www.therobotreport.com/nvidias-rvt-can-learn-new-tasks-after-just-10-demos/"><img src='https://www.therobotreport.com/wp-content/uploads/2018/01/the-robot-report-business-of-robots-Lisa-Eitel-Dan-Kara-Steve-Crowe.jpg' height=45px></a> 
                <a href="https://marktechpost-newsletter.beehiiv.com/p/ai-news-baidu-challenges-gpt-theorem-proving-llms-motiongpt-ai-updates-june-28-2023-edition"><img src='https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/publication/logo/f5e63dd4-5653-4f09-83e2-321a8b1ba526/thumb_AI_PAPERS_SUMMARIES.png' height=45px></a> 


		<p align="justify"> RVT is a multi-view transformer for 3D manipulation that is both scalable and accurate.  In simulations, a single RVT model works well across 18 RLBench tasks with 249 task variations, achieving 26% higher relative success than existing  SOTA (PerAct). It also trains 36X faster than PerAct for achieving the same performance and achieves 2.3X the inference speed of PerAct. Further, RVT can perform a variety of manipulation tasks in the real world with just a few (~10) demonstrations per task.</p>
            </td>
          </tr>

	  <!-- RPDiff -->
          <tr class="publication-row">
              <td width="35%">
		    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="https://anthonysimeonov.github.io/rpdiff-multi-modal/img/book_bookshelf_seq1_8x_rdc_lout.mp4" type="video/mp4">
                </video>
              <!-- <iframe src="https://anthonysimeonov.github.io/rpdiff-multi-modal/img/book_bookshelf_seq1_8x_rdc_lout.mp4" -->
              <!--     frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
              <!-- <img src='https://anthonysimeonov.github.io/rpdiff-multi-modal/img/book_bookshelf_seq1_8x_rdc_lout.mp4' width=100%> -->
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://anthonysimeonov.github.io/rpdiff-multi-modal/">
                  <papertitle>Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement</papertitle>
                </a>
                <br>
		Anthony Simeonov, Ankit Goyal, Lucas Manuelli, Lin Yen-Chen, Alina Sarmiento, Alberto Rodriguez, Pulkit Agrawal, Dieter Fox
                <br>
		<em>CoRL 2023</em> 
		<br>
		<a href="https://github.com/anthonysimeonov/rpdiff">[code]</a> 
		<a href="https://anthonysimeonov.github.io/rpdiff-multi-modal/">[project page]</a> 
		<a href="https://www.youtube.com/watch?v=x9noTl_aqu0">[video]</a> 
		<p align="justify"> RPDiff rearranges objects into "multimodal" configurations, such as a book inserted in an open slot of a bookshelf. It generalizes to novel geometries, poses, and layouts, and is trained from demonstrations to operate on point clouds.</p>
            </td>
          </tr>

	  <!-- Infinigen -->
          <tr class="publication-row">
              <td width="35%">
              <img src='https://infinigen.org/img/random_sample.jpeg' width=100%>
              <!-- <iframe src="https://www.youtube.com/embed/6tgspeI-GHY?rel=0&amp;autoplay=1&mute=1&vq=small"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen loading="lazy"></iframe> -->
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://infinigen.org/">
                  <papertitle>Infinite Photorealistic Worlds using Procedural Generation</papertitle>
                </a>
                <br>
		Alexander Raistrick, Lahav Lipson, Zeyu Ma, Lingjie Mei, Mingzhe Wang, Yiming Zuo, Karhan Kayan, Hongyu Wen, Beining Han, Yihan Wang, Alejandro Newell, Hei Law, <strong>Ankit Goyal</strong>, Kaiyu Yang, and Jia Deng
                <br>
		<em>CVPR 2023</em> 
		<br>
		<a href="https://infinigen.org/">[project page]</a> 
    <br> <br>
                <a href="https://news.ycombinator.com/item?id=36376071"><img src='https://news.ycombinator.com/y18.svg' height=45px></a>

		<p align="justify">Infinigen is a generator of unlimited high-quality 3D data. Procedural and open-source.</p>
            </td>
          </tr>

	  <!-- ProgPrompt -->
          <tr class="publication-row">
              <td width="35%">
              <img src='images/progprompt.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2209.11302">
                  <papertitle>ProgPrompt: Generating Situated Robot Task Plans using Large Language Models</papertitle>
                </a>
                <br>
		Ishika Singh, Valts Blukis, Arsalan Mousavian, <strong>Ankit Goyal</strong>, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg 
                <br>
		<em>ICRA 2023</em> 
		<br>
		Also in <em>Autonomous Robots</em>, <em>LaRel @ NeurIPS 2022</em> and <em>LangRob @ CoRL 2022</em>
		<br>
		<a href="https://progprompt.github.io/">[project page]</a> 
		<p align="justify">We use large language models (LLMs) for task planning in robotics. We construct pythonic prompts, which specify the task, robot capabilities and the environment to seed LLMs.</p>
            </td>
          </tr>
	  <!-- IFOR -->

          <tr class="publication-row">
            <td width="35%">
              <img src='images/ifor.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2202.00732">
                  <papertitle>IFOR: Iterative Flow Minimization for Robotic Object Rearrangement</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Arsalan Mousavian, Chris Paxton, Yu-Wei Chao, Brian Okorn, Jia Deng, Dieter Fox
                <br>
		<em>CVPR 2022</em> 
		<br>
		Also in <em>EAI @ CVPR 2022</em>
		<br>
		<a href="https://imankgoyal.github.io/ifor.html">[project page]</a> 
		<a href="data/ifor_slides.pdf">[slides]</a> 
		<a href="data/ifor_poster.pdf">[poster]</a> 
		<p align="justify">IFOR is an end-to-end method for the challenging problem of object rearrangement for unknown objects given an RGBD image of the original and final scenes. It works on cluttered scenes in the real world, while training only on synthetic data. </p>
            </td>
          </tr>

	  <!-- 6DOF -->
          <tr class="publication-row">
            <td width="35%">
              <img src='images/6dof.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2204.12516">
                  <papertitle>Coupled Iterative Refinement for 6D Multi-Object Pose Estimation</papertitle>
                </a>
                <br>
                Lahav Lipson, Zachary Teed, <strong>Ankit Goyal</strong>, Jia Deng
                <br>
                <em>CVPR 2022</em>               
		<br>
		<a href="https://arxiv.org/abs/2204.12516">[paper]</a> <a href="https://github.com/princeton-vl/Coupled-Iterative-Refinement">[code]</a><br> 
		<p align="justify">We propose state-of-the-art 6DOF multi-object pose estimation system. Our system iteratively refines object pose and correspondece.</p>
            </td>
          </tr>

	  <!-- Non-deep Networks -->
          <tr class="publication-row">
              <td width="35%">
              <img src='images/nondeep.jpg' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2110.07641">
                  <papertitle>Non-deep Networks</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Alexey Bochkovskiy, Jia Deng, Vladlen Koltun
                <br>
                <em>NeurIPS 2022</em>                
		<br>
		<a href="https://github.com/imankgoyal/NonDeepNetworks">[code]</a> 
		<a href="data/non_deep_poster.pdf">[poster]</a> 
		<a href="data/non_deep_conf.key">[slides]</a> 
		<a href="https://neurips.cc/virtual/2022/poster/55098">[video]</a> 
		<br> <br>
                <a href="https://syncedreview.com/2021/10/22/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-129/"><img src='images/synced.png' height=25px></a> &nbsp; &nbsp;
                <a href="https://papersread.ai/e/non-deep-networks/"><img src='images/podbean.png' height=25px></a>                 
		<p align="justify"> Depth is the hallmark of DNNs. But more depth means more sequential computation and higher latency. This begs the question -- is it possible to build high-performing ``non-deep" neural networks? We show it is. </p>
            </td>
          </tr>
	
	  <!-- SimpleView -->
          <tr class="publication-row">
              <td width="35%">
              <img src='images/simpleview.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2106.05304">
                  <papertitle>Revisiting Point Cloud Shape Classification with a Simple and Effective Baseline</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Hei Law, Bowei Liu, Alejnadro Newell, Jia Deng 
                <br>
                <em>ICML</em> 2021                
		<br>
                <a href="https://github.com/princeton-vl/SimpleView">[code]</a> 
		<a href="https://docs.google.com/presentation/d/1eYKDth19S2Ovt1lO0CpbzCM_hh7QEVni/edit?usp=drive_link&ouid=111606148346153000949&rtpof=true&sd=true">[slides]</a>
		<a href="https://drive.google.com/file/d/1TZIbcEaSQmjyOUiURYVzYthduzui1Rcx/view?usp=drive_link">[poster]</a>
		<a href="https://icml.cc/virtual/2021/poster/9099">[video]</a>
                <p align="justify"> Many point-based approaches have been proposed reporting steady benchmark improvements over time. We study the key ingredients of this progress and uncover two critical results. First, auxiliary factors, independent of the model architecture, make a large difference in performance. Second, a very simple projection based method performs surprisingly well. </p>
            </td>
          </tr>
          
          <!-- Rel3D -->
          <tr class="publication-row">
              <td width="35%">
              <img src='images/rel3D.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://papers.nips.cc/paper/2020/hash/76dc611d6ebaafc66cc0879c71b5db5c-Abstract.html">
                  <papertitle>Rel3D: A Minimally Contrastive Benchmark for Grounding Spatial Relations in 3D</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Kaiyu Yang, Dawei Yang, <a href = "http://www.cs.princeton.edu/~jiadeng/"> Jia Deng</a>
                <br>
                <em>NeuRIPS</em> 2020, <span style="color:brown;">Spotlight (Top 4% of submitted papers)</span>                
		<br>
                <a href="https://github.com/princeton-vl/Rel3D">[code]</a> 
		<a href="https://drive.google.com/file/d/1lWA2WlJR4itJJrnHMRiZ87-z8akagkH4/view?usp=sharing">[slides]</a>
		<a href="data/rel3d_poster.pdf">[poster]</a>
		<a href="https://slideslive.com/38935853/rel3d-a-minimally-contrastive-benchmark-or-grounding-spatial-relations-in-3d">[video]</a>
                <p align="justify"> Understanding spatial relations is important for both humans and robots. We create Rel3D, the first large-scale, human-annotated dataset for grounding spatial relations in 3D.  The 3D scenes in Rel3D come in minimally contrastive pairs: two scenes in a pair are almost identical, but a spatial relation holds in one and fails in the other. </p>
            </td>
          </tr>

          <!-- PackIt -->
          <tr class="publication-row">
              <td width="35%">
              <img src='images/packit.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2007.11121">
                  <papertitle>PackIt: A Virtual Environment for Geometric Planning</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, <a href = "http://www.cs.princeton.edu/~jiadeng/"> Jia Deng</a>
                <br>
                <em>ICML</em> 2020
                <br>
                <a href="https://github.com/princeton-vl/PackIt">[code]</a>
                <a href="data/packit_slides.pptx">[slides]</a>
		<a href="https://slideslive.com/38927501/packit-a-virtual-environment-for-geometric-planning?ref=search">[video]</a>
                <p align="justify"> Simultaneously reasoning about geometry and planning action is crucial for intelligent agents. This ability of geometric planning comes in handy while grocery shopping, rearranging room, warehouse management etc. We create PackIt, a virtual environment that caters to geometric planning.</p>
            </td>
          </tr>

          <!-- Think Visually -->
          <tr class="publication-row">
              <td width="35%">
              <img src='images/dsmn2.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/1805.11025">
                  <papertitle>Think Visually: Question Answering through Virtual Imagery</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Jian Wang, <a href = "http://www.cs.princeton.edu/~jiadeng/"> Jia Deng</a>
                <br>
                <em>ACL</em> 2018
                <br>
                <a href="https://github.com/princeton-vl/think_visually">[code]</a>
                <a href="data/think_visually_poster.pdf">[poster]</a>
                <p align="justify">We study geometric reasoning in the context of question-answering. We introduce Dynamic Spatial Memory Network (DSMN), a deep network architecture designed for answering questions that admit latent visual representations. </p>
            </td>
          </tr>

          <!-- ProtoNN -->
          <tr class="publication-row">
              <td width="35%">
              <img src='images/protonn.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <a href="http://proceedings.mlr.press/v70/gupta17a.html">
                <papertitle>ProtoNN: Compressed and Accurate kNN for Resource-scarce Devices</papertitle>
              </a>
              <br>
              C Gupta, AS Suggala,<strong> A Goyal</strong>, HV Simhadri, BP, AK, SG, RU, MV, <a href="https://www.microsoft.com/en-us/research/people/prajain/">P Jain</a>
              <br>
              <em>ICML</em> 2017
              <br>
              <a href="https://github.com/Microsoft/EdgeML">[code]</a>
              <br><br>
              <a href="https://patents.google.com/patent/US20180330275A1/en">
                <papertitle>Resource-Efficient Machine Learning</papertitle>
              </a>
              <br>
              Prateek Jain, Chirag Gupta, AS Suggala,<strong> Ankit Goyal</strong>, HV Simhadri
              <br>
              <em>US Patent Applicaiton</em>
              <br>
              <p align="justify">We propose ProtoNN, a novel algorithm that addresses the problem of real-time and accurate prediction on resource-scarce devices. </p>
             
            </td>
          </tr>

          <!-- Emotion Prediction -->
          <tr class="publication-row">
              <td width="35%">
              <img src='images/multimodal.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <a href="https://ieeexplore.ieee.org/abstract/document/7472192/">
                <papertitle>A Multimodal Mixture-Of-Experts Model for Dynamic Emotion Prediction in Movies</papertitle>
              </a>
              <br>
              <strong>Ankit Goyal</strong>, <a href="https://algoseer.github.io/">Naveen Kumar</a>, <a href="http://tanayag.com/Home.html">Tanaya Guha</a>, <a href="https://sail.usc.edu/people/shri.php">Shrikanth S. Narayanan</a>
              <br>
              <em>ICASSP</em> 2016
              <br>
              <p align="justify">We address the problem of continuous emotion prediction in movies. We propose a Mixture of Experts (MoE)-based fusion model that dynamically combines information from the audio and video modalities for predicting the emotion evoked in movies. </p>
              
            </td>
          </tr>

          <!-- <!-1- SURF -1-> -->
          <!-- <tr> -->
          <!--   <td width="35%"> -->
          <!--     <img src='images/surf.png' width=100%> -->
          <!--   </td> -->
          <!--   <td valign="top" width="70%"> -->
          <!--     <a href="https://link.springer.com/chapter/10.1007/978-3-319-27000-5_34"> -->
          <!--       <papertitle>Object Matching Using Speeded Up Robust Features</papertitle> -->
          <!--     </a> -->
          <!--     <br> -->
          <!--     NK Verma, <strong> Ankit Goyal</strong>, A Harsha Vardhan, Rahul Kumar Sevakula, Al Salour -->
          <!--     <br> -->
          <!--     <em>IES</em> 2016 -->
          <!--     <br> -->
          <!--     <p align="justify"> We propose a robust algorithm which is capable of detecting all the instances of a particular object in a scene image using Speeded Up Robust Features. </p> -->
          <!--   </td> -->
          <!-- </tr> -->

          <!-- <!-1- Template Matching -1-> -->
          <!-- <tr> -->
          <!--   <td width="35%"> -->
          <!--     <img src='images/template.png' width=100%> -->
          <!--   </td> -->
          <!--   <td valign="top" width="70%"> -->
          <!--     <p> -->
          <!--       <a href="https://ieeexplore.ieee.org/abstract/document/7334132/"> -->
          <!--         <papertitle>Template Matching for Inventory Management using Fuzzy Color Histogram and Spatial Filters</papertitle> -->
          <!--       </a> -->
          <!--       <br> -->
          <!--       NK Verma,<strong> Ankit Goyal</strong>, Anadi Chaman, Rahul K Sevakula, Al Salour -->
          <!--       <br> -->
          <!--       <em>ICIEA</em> 2015 -->
          <!--       <br> -->
          <!--       <p align="justify">We propose a methodology for object counting using color histogram based segmentation and spatial filters. </p> -->
          <!--    </p> --> 
          <!--   </td> -->
          <!-- </tr> -->

        </table>

        <!-- Teaching -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td width="100%" valign="middle">
                <heading>Teaching</heading>
                <p>
                  I have been a Teaching Assistant for the following courses:
                    <ul>
                      <li> COS529: Advanced Computer Vision at Princeton University [Winter 2020] </li>
                      <li> COS429: Computer Vision at Princeton University [Fall 2018] </li>
                      <li> EECS442: Computer Vision at University of Michigan  [Fall 2017, Winter 2018]</li>
                    </ul>
                </p>
<!-- 		Some <a href="https://imankgoyal.github.io/related.html">related papers</a> to mine. -->
              </td>
            </tr>
        </table>

        <!-- Reference -->
        <p align="right"><a href="https://jonbarron.info/">[Web Cite]</a></p>
      </td>
    </tr>
  </table>  

</body>

</html>
