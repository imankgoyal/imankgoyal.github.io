<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-129673183-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-129673183-1');
</script>

  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
      transition: all 0.2s ease;
      position: relative;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    /* Subtle underline effect for links in paragraphs */
    p a:not([href*="mailto"]):not([href*="twitter"]):hover {
      text-decoration: underline;
      text-decoration-color: rgba(240, 146, 40, 0.3);
      text-underline-offset: 2px;
    }
    
    body {
      font-family: 'Outfit', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      line-height: 1.6;
      background: linear-gradient(to bottom, #fdfbf7 0%, #f9f6f2 50%, #fdfbf7 100%);
      background-attachment: fixed;
      color: #3a3a3a;
    }
    
    td,
    th,
    tr,
    p {
      font-family: 'Outfit', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      line-height: 1.6;
      color: #3a3a3a;
    }
    
    a {
      font-family: 'Outfit', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      line-height: 1.6;
    }
    
    strong {
      font-family: 'Outfit', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Outfit', Verdana, Helvetica, sans-serif;
      font-size: 24px;
      font-weight: 600;
      color: #4a4a4a;
      display: block;
      margin-bottom: 10px;
    }
    
    papertitle {
      font-family: 'Outfit', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700;
    }
    
    /* Enhanced paper title link styling */
    a papertitle {
      transition: all 0.2s ease;
    }
    
    a:hover papertitle {
      letter-spacing: 0.3px;
    }
    
    name {
      font-family: 'Outfit', Verdana, Helvetica, sans-serif;
      font-size: 34px;
      font-weight: 700;
      color: #2a2a2a;
      letter-spacing: -0.5px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
    
    /* Publication row hover effect */
    .publication-row {
      transition: all 0.3s ease;
      border-radius: 8px;
      padding: 10px;
    }
    
    .publication-row:hover {
      transform: translateY(-3px);
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
      background-color: rgba(250, 248, 245, 0.8);
    }
    
    /* Better paragraph spacing */
    p {
      margin-bottom: 12px;
    }
    
    /* Section spacing */
    table[cellpadding="20"] {
      margin-bottom: 5px;
    }
    
    /* List styling */
    ul {
      line-height: 1.8;
    }
    
    li {
      margin-bottom: 8px;
    }
    
    /* News section styling */
    .news-section {
      background-color: rgba(250, 248, 245, 0.9);
      border-radius: 8px;
      padding: 20px;
    }
    
    .news-section ul {
      list-style-type: none;
      padding-left: 0;
    }
    
    .news-section li {
      padding: 10px 15px;
      margin-bottom: 10px;
      border-left: 3px solid #1772d0;
      background-color: rgba(255, 254, 251, 0.95);
      border-radius: 4px;
      transition: all 0.2s ease;
    }
    
    .news-section li:hover {
      transform: translateX(5px);
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
      border-left-color: #f09228;
    }
    
    /* Details/Summary styling for older news */
    details.details-example {
      margin-top: 15px;
    }
    
    details.details-example summary {
      cursor: pointer;
      padding: 10px 15px;
      background-color: rgba(23, 114, 208, 0.05);
      border-radius: 4px;
      font-weight: 600;
      color: #1772d0;
      transition: all 0.2s ease;
    }
    
    details.details-example summary:hover {
      background-color: rgba(23, 114, 208, 0.1);
    }
    
    details.details-example[open] summary {
      margin-bottom: 10px;
    }
    
    /* Institution timeline styling */
    .institution-timeline td:hover {
      background-color: rgba(250, 248, 245, 0.9) !important;
    }
    
    .institution-timeline img {
      transition: all 0.3s ease;
      filter: grayscale(0%);
      opacity: 0.9;
    }
    
    .institution-timeline img:hover {
      filter: grayscale(0%);
      opacity: 1;
      transform: scale(1.05);
    }
    
    /* Publication media styling */
    .publication-row img,
    .publication-row video {
      border-radius: 6px;
      transition: all 0.3s ease;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
      border: 1px solid rgba(0, 0, 0, 0.05);
    }
    
    .publication-row:hover img,
    .publication-row:hover video {
      box-shadow: 0 4px 16px rgba(0, 0, 0, 0.12);
      transform: scale(1.02);
    }
    
    /* Smooth appearance for all images */
    img {
      image-rendering: -webkit-optimize-contrast;
      image-rendering: crisp-edges;
    }
    
    /* Contact links bar styling - Connected segments */
    .contact-links {
      padding: 20px;
      margin-top: 20px;
      margin-bottom: 20px;
      display: flex;
      justify-content: center;
      align-items: center;
    }
    
    .contact-links a {
      padding: 10px 20px;
      margin: 0;
      border: 1.5px solid rgba(23, 114, 208, 0.3);
      border-right: none;
      transition: all 0.2s ease;
      display: inline-block;
      background-color: rgba(255, 254, 251, 0.9);
      font-weight: 500;
      color: #1772d0;
    }
    
    .contact-links a:first-of-type {
      border-radius: 8px 0 0 8px;
    }
    
    .contact-links a:last-of-type {
      border-radius: 0 8px 8px 0;
      border-right: 1.5px solid rgba(23, 114, 208, 0.3);
    }
    
    .contact-links a:hover {
      background-color: rgba(23, 114, 208, 0.1);
      color: #f09228 !important;
      z-index: 1;
      position: relative;
    }
    
    /* Profile image styling */
    .profile-image {
      border-radius: 8px;
      box-shadow: 0 4px 16px rgba(0, 0, 0, 0.1);
      border: 1px solid rgba(0, 0, 0, 0.06);
      transition: all 0.3s ease;
    }
    
    .profile-image:hover {
      transform: translateY(-3px);
      box-shadow: 0 8px 24px rgba(0, 0, 0, 0.15);
    }
    
    /* Asymmetric intro layout */
    .asymmetric-intro .name-header {
      text-align: center;
      padding-bottom: 20px;
      margin-bottom: 20px;
      transition: all 0.3s ease;
      border-radius: 8px;
      padding: 15px;
    }
    
    .asymmetric-intro .name-header:hover {
      transform: translateY(-2px);
      background-color: rgba(250, 248, 245, 0.6);
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
    }
    
    .asymmetric-intro .content-row {
      display: flex;
      align-items: center;
      gap: 30px;
    }
    
    .asymmetric-intro .bio-content {
      flex: 2.2;
      transition: all 0.3s ease;
      border-radius: 8px;
      padding: 15px;
    }
    
    .asymmetric-intro .bio-content:hover {
      transform: translateY(-3px);
      background-color: rgba(250, 248, 245, 0.6);
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.06);
    }
    
    .asymmetric-intro .image-content {
      flex: 1;
      display: flex;
      align-items: flex-start;
      justify-content: center;
      padding-top: 0;
    }
    
    /* Main content container */
    body > table {
      background-color: rgba(255, 254, 251, 0.95);
      box-shadow: 0 0 30px rgba(0, 0, 0, 0.08);
      border-radius: 10px;
      margin-top: 20px;
      margin-bottom: 20px;
      padding: 20px;
    }
    
    /* Section spacing improvements */
    heading {
      margin-top: 10px;
    }
    
    /* Media coverage brick styles */
    .media-bricks {
      display: grid;
      grid-template-columns: repeat(4, 1fr);
      gap: 20px;
      padding: 0 20px;
    }
    
    .media-brick-item {
      text-decoration: none;
      display: flex;
      flex-direction: column;
      align-items: center;
      text-align: center;
      max-width: 180px;
      padding: 12px;
      background-color: rgba(250, 248, 245, 0.6);
      border: 1px solid rgba(0, 0, 0, 0.06);
      border-radius: 10px;
      transition: background-color 0.2s ease;
    }
    
    .media-brick-item:hover {
      background-color: rgba(250, 248, 245, 0.9);
    }
    
    .media-brick-box {
      padding: 12px 16px;
      border-radius: 8px;
      display: flex;
      align-items: center;
      justify-content: center;
      width: auto;
      min-width: 130px;
      transition: transform 0.3s cubic-bezier(0.34, 1.56, 0.64, 1), box-shadow 0.3s ease;
      cursor: pointer;
    }
    
    .media-brick-box:hover {
      transform: scale(1.08) translateY(-4px);
      box-shadow: 0 8px 20px rgba(0, 0, 0, 0.25);
    }
    
    .media-brick-box span {
      color: white;
      font-size: 14px;
      font-weight: 600;
    }
    
    .media-brick-paper {
      color: #1772d0;
      font-size: 12px;
      font-weight: 600;
      margin-top: 8px;
    }
    
    .media-brick-desc {
      color: #666;
      font-size: 11px;
      margin-top: 4px;
      line-height: 1.4;
    }
  </style>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><rect width='100' height='100' fill='%231772d0'/><text x='50' y='50' font-size='45' font-weight='bold' text-anchor='middle' dominant-baseline='middle' fill='white' font-family='Arial'>AG</text></svg>">
  <title>Ankit Goyal</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css2?family=Outfit:wght@400;500;600;700&display=swap' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <!-- Bio and Image -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" class="asymmetric-intro">
          <tr>
            <td width="100%" class="name-header">
                <name>Ankit Goyal</name>
            </td>
          </tr>
          <tr>
            <td width="100%">
              <div class="content-row">
                <div class="bio-content">
                  <p align="justify">
		    I am a Senior Research Scientist at NVIDIA, working on generalist robot learning. My research establishes the <span style="color:#8b5cf6; font-weight:500;">foundations</span> for <span style="color:#d97706; font-weight:500;">generalist robot learning</span> in the era of <span style="color:#059669; font-weight:500;">foundation models</span>.
	          </p>
              <p align="justify">
		    I completed my Ph.D. in CS at Princeton University, advised by <a href="http://www.cs.princeton.edu/~jiadeng/">Prof. Jia Deng</a>, and have been fortunate to work with pioneers in AI including <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a>, <a href="http://vladlen.info/">Vladlen Koltun</a>, <a href="https://www.prateekjain.org/">Prateek Jain</a>, and <a href="https://sail.usc.edu/people/shri.php">Shrikanth Narayanan</a>. I am an <a href='https://sites.google.com/view/rsspioneers2023/participants?authuser=0'>RSS Pioneer</a> and a <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2021-north-america">Qualcomm Innovation Fellow</a>.
	          </p>
	      <p align="justify"> 
		    A common thread in my work is finding the simplest possible solution to complex problems. My contributions span robot learning, manipulation, and 3D Vision. I introduced <a href="https://vla0.github.io/">VLA-0</a>, the simplest possible vision-language-action model, and <a href="https://hamster-robot.github.io/">HAMSTER</a>, a hierarchical VLA for complex tasks. My works, <a href="https://robotic-view-transformer.github.io/">RVT</a> and <a href="https://robotic-view-transformer-2.github.io/">RVT-2</a>, have been widely adopted as a core component for state-of-the-art 3D manipulation systems. I have also contributed to <a href="https://progprompt.github.io/">ProgPrompt</a>, now a standard in code-as-planning pipelines, and <a href="https://infinigen.org/">Infinigen</a>, a generator of infinite photorealistic 3D worlds.
	          </p>
	          <p align="justify">
		    My work has been covered by <a href="https://news.mit.edu/2025/new-system-enables-robots-to-solve-manipulation-problems-seconds-0605">MIT News</a>, <a href="https://news.ycombinator.com/item?id=36376071">Hacker News</a>, <a href="https://www.therobotreport.com/nvidias-rvt-can-learn-new-tasks-after-just-10-demos/">The Robot Report</a>, and <a href="https://blogs.nvidia.com/blog/ai-research-iclr-2025/">NVIDIA Blog</a>. My open-source code has cumulatively garnered over 1,500 stars on GitHub (<a href="https://github.com/NVlabs/vla0">VLA-0</a>, <a href="https://github.com/nvlabs/rvt">RVT</a>, <a href="https://github.com/imankgoyal/NonDeepNetworks">Non-Deep Networks</a>, and <a href="https://github.com/princeton-vl/SimpleView">SimpleView</a>) and is used by researchers worldwide.
	          </p>
              </div>
                <div class="image-content">
                  <img src="images/self_rectangular.png" class="profile-image" style="width: 100%; max-width: 260px;">
                </div>
              </div>
            </td>
          </tr>
	  <tr>
            <td colspan="2">
              <p align=center margin=0px class="contact-links">
                <a href="mailto:ankgoyal@umich.edu">Email</a>
                <a href="data/AnkitGoyal_CV.pdf">CV</a>
                <a href="https://scholar.google.com/citations?user=RhN6jKIAAAAJ&hl=en&oi=sra">Scholar</a>
                <a href="https://github.com/imankgoyal">Github</a>
                <a href="https://www.linkedin.com/in/ankit-goyal-5baaa287/">LinkedIn</a>
                <a href="https://x.com/imankitgoyal">Twitter</a>
             </p>
	   </td>
	  </tr>
        </table>
	<div class = "row">
        <table width="100%" align="center" margin-top=100px class="institution-timeline">
            <tr>
              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/nvidia.png" width="50%">
              </td>

              <td align="center" width="16%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/princeton.png" width="30%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/intel.png" width="60%">
              </td>

              <td align="center" width="16%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/michigan.png" width="50%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/msr.png" width="90%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/usc.png" width="30%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/iitk.png" width="60%">
              </td>
            </tr>

            <tr>
                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Scientist<br>NVIDIA<br>Current</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">PhD, CS<br>Princeton University<br>2018 - 2022</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Intern<br>Intel<br>Winter 2021</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">MS, CSE<br>University of Michigan<br>2016 - 2018</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Intern<br>MSR<br>Summer 2016</td>
		
                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Intern<br>USC<br>Summer 2015</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">BTech, EE<br>IIT Kanpur<br>2012 - 2016</td>
          </tr>

          </table>
      </div>

        <!-- Recent News Section -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td width="100%" valign="middle">
        <heading>Recent News</heading>
      </td>
    </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="top" class="news-section">
              <ul>
                <li><i>Dec 2025:</i> Gave invited talks at <a href="https://calendars.illinois.edu/detail/6991/33536017">UIUC Computer Vision Seminar</a> and JHU.</li>
                <li><i>Oct 2025:</i> Released <a href="https://vla0.github.io/">VLA-0</a>, a simple yet state-of-the-art approach to building Vision-Language-Action models.</li>
                <li><i>Oct 2025:</i> Gave an invited talk at <a href="https://opendrivelab.com/iccv2025/workshop/">ICCV 2025 Workshop on Advancing Spatial Understanding for Embodied Intelligence</a>.</li>
                <li><i>Oct 2025:</i> Gave a guest lecture at <a href="https://labs.utdallas.edu/irvl/courses/fall-2025-cs-6341-robotics/">UT Dallas CS 6341 Robotics</a> on "Perspectives on Designing Vision-Language-Action Models".</li>
                <li><i>Sep 2025:</i> <a href="https://maniflow-policy.github.io/">ManiFlow</a> was accepted to CoRL 2025.</li>
                <li><i>Jun 2025:</i> <a href="https://cutamp.github.io/">cuTAMP</a> was accepted to RSS 2025 and covered by <a href="https://news.mit.edu/2025/new-system-enables-robots-to-solve-manipulation-problems-seconds-0605">MIT News</a>.</li>
                <li><i>Feb 2025:</i> <a href="https://jasonqsy.github.io/3DMVP/">3D-MVP</a>, 3D pretraining for manipulation was accepted to CVPR 2025.</li>
                <li><i>Jan 2025:</i> <a href="https://hamster-robot.github.io/">HAMSTER</a>, a hierarchical VLA for open-world manipulation was accepted to ICLR 2025.</li>
              </ul>
              <details class="details-example">
                <summary>Older news</summary>
                <ul>
                  <li><i>Sep 2024:</i> Gave a talk at <a href='https://www.youtube.com/watch?v=E_vOQsXWA30/'>MILA Robot Learning Seminar</a></li>
                  <li><i>Aug 2024:</i> <a href="https://actaim2.github.io">ActAIM2</a>, which discovers self-supervised action modes accepted to CoRL 2024.</li>
                  <li><i>May 2024:</i> <a href="https://robotic-view-transformer-2.github.io/">RVT-2</a> accepted to RSS 2024.</li>
                  <li><i>May 2024:</i> Gave a keynote talk at <a href="https://icra-manipulation-skill.github.io/"> Manipulation Skills Workshop in ICRA</a>.</li>
                  <li><i>Oct 2023:</i> Gave a talk at <a href="https://robotics.cs.washington.edu/colloquium/"> UW Robotics Colloquium </a> along with Caelan Garrett and Iretiayo Akinola.</li>
                  <li><i>Aug 2023:</i> Two papers (including one Oral) accepted to CoRL 2023. <a href="https://robotic-view-transformer.github.io/">RVT</a> and <a href="https://github.com/anthonysimeonov/rpdiff">"Shelving, Stacking and Hanging"</a>.</li>
                  <li><i>June 2023:</i> Released <a href="https://robotic-view-transformer.github.io/">Robotic View Transformer</a> for fast and performant 3D manipulation.</li>
                  <li><i>May 2023:</i> Selected to be a part of the <a href='https://sites.google.com/view/rsspioneers2023/participants?authuser=0'>RSS Pioneers Cohort, 2023</a>.</li>
                  <li><i>Apr 2023:</i> Gave a talk at <a href='https://mila-vision-rg.github.io/'>MILA Vision Reading Group</a>. Thanks for the invite!</li>
                  <li><i>Feb 2023:</i> <a href="https://arxiv.org/abs/2209.11302">ProgPrompt</a> accepted to <b>ICRA 2023</b> - LLM+Robotics led by <a href="https://ishikasingh.github.io/">Ishika</a> and <a href="https://www.cs.cornell.edu/~valts/">Valts</a>.</li>
                  <li><i>Oct 2022:</i> <a href='https://github.com/princeton-vl/Coupled-Iterative-Refinement'>6D pose estimation work</a> led by <a href='https://www.lahavlipson.com/'>Lahav Lipson</a> won an award in ECCV BOP challenge 2022.</li>
                  <li><i>Sep 2022:</i> Received the <a href='https://neurips.cc/Conferences/2022/FinancialAssistance'> NeurIPS Scholar Award</a>.</li>
                  <li><i>Sep 2022:</i> Paper on <a href="https://arxiv.org/abs/2110.07641">non-deep networks</a> accepted to <b>NeurIPS 2022</b>.</li>
                  <li><i>Aug 2022:</i> Defended my Ph.D. thesis and started as a Research Scientist at NVIDIA working with <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a>.</li>
                  <li><i>Mar 2022:</i> Two papers accepted to <b>CVPR 2022</b>.</li>
                  <li><i>Aug 2021:</i> Recognized as an <a href="http://iccv2021.thecvf.com/outstanding-reviewers">outstanding reviewer</a> at <b>ICCV 2021</b>.</li>
                  <li><i>July 2021:</i> Selected for <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2021-north-america">Qualcomm Innovation Fellowship</a> with Zachary Teed!</li>
                </ul>
              </details>
            </td>
          </tr>
        </table>

        <!-- Research Interest -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td width="100%" valign="middle">
        <heading>Selected Publications</heading>
      </td>
    </tr>
        </table>

        <!-- Selected Publications-->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    
    <!-- VLA-0 -->
          <tr class="publication-row">
              <td width="35%">
                  <video width="100%" autoplay loop muted>
                      <source src="https://vla0.github.io/Videos/teaser.mp4" type="video/mp4">
                  </video>
              </td>
              <td valign="top" width="70%">
                  <p>
                      <a href="https://vla0.github.io/">
                          <papertitle>VLA-0: Building State-of-the-Art VLAs with Zero Modification</papertitle>
                      </a>
                      <br>
                      <strong>Ankit Goyal</strong>, Hugo Hadfield, Xuning Yang, Valts Blukis, Fabio Ramos
                      <br>
                      <em>arXiv 2025</em> 
                      <br>
                      <a href="https://vla0.github.io/">[project page]</a> 
                      <a href="https://arxiv.org/abs/2510.13054">[paper]</a>
                      <p align="justify">
                        We introduce VLA-0, a surprisingly simple approach to building Vision-Language-Action models that achieves state-of-the-art results by representing actions directly as text, without any architectural changes to the base VLM. VLA-0 outperforms all methods trained on the same robotic data on LIBERO benchmark and even surpasses models with large-scale pretraining.
                      </p>
                  </p>
              </td>
          </tr>

    <!-- ManiFlow -->
          <tr class="publication-row">
              <td width="35%">
                  <video width="100%" autoplay loop muted>
                      <source src="images/bunny_stack_1_4x.mp4" type="video/mp4">
                  </video>
              </td>
              <td valign="top" width="70%">
                  <p>
                      <a href="https://maniflow-policy.github.io/">
                          <papertitle>ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training</papertitle>
                      </a>
                      <br>
                      Ge Yan, Jiyue Zhu*, Yuquan Deng*, Shiqi Yang, Ri-Zhao Qiu, Xuxin Cheng, Marius Memmel, Ranjay Krishna^, <strong>Ankit Goyal</strong>^, Xiaolong Wang^, Dieter Fox^
                      <br>
                      <em>CoRL 2025</em> 
                      <br>
                      <a href="https://maniflow-policy.github.io/">[project page]</a>
                      <a href="https://github.com/geyan21/ManiFlow_Policy">[code]</a>
                      <a href="https://arxiv.org/abs/2509.01819">[paper]</a>
                      <p align="justify">
                        ManiFlow is a visuomotor imitation learning policy for general robot manipulation that generates precise, high-dimensional actions in just 1-2 inference steps using flow matching with consistency training.
                      </p>
                  </p>
              </td>
          </tr>
    
    <!-- HAMSTER -->
          <tr class="publication-row">
              <td width="35%">
                  <video width="100%" autoplay loop muted>
                      <source src="https://hamster-robot.github.io/figs/hamster_teaser_v2.mp4" type="video/mp4">
                  </video>
              </td>
              <td valign="top" width="70%">
                  <p>
                      <a href="https://hamster-robot.github.io/">
                          <papertitle>HAMSTER: Hierarchical Action Models for Open-World Manipulation</papertitle>
                      </a>
                      <br>
                      Yi Li*, Yuquan Deng*, Jesse Zhang*, Joel Jang, Marius Memmel, Raymond Yu, Caelan Reed Garrett, Fabio Ramos, Dieter Fox, Anqi Li^, Abhishek Gupta^, <strong>Ankit Goyal</strong>^
                      <br>
                      <em>ICLR 2025</em> 
                      <br>
                      <a href="https://github.com/liyi14/HAMSTER_beta">[code]</a> 
                      <a href="https://hamster-robot.github.io/">[project page]</a> 
                      <p align="justify">
                        We introduce HAMSTER, a Hierarchical Vision-Language-Action (VLA) architecture designed for robotic manipulation. This approach effectively combines the advantages of imitation learning models, which require little in-domain robot data, with those of large VLA models that can generalize well.
                      </p>
                  </p>
              </td>
          </tr>
    
    <!-- 3D-MVP -->
          <tr class="publication-row">
              <td width="35%">
                  <img src='https://jasonqsy.github.io/3DMVP/static/images/architecture.png' width=100%>
              </td>
              <td valign="top" width="70%">
                  <p>
                      <a href="https://jasonqsy.github.io/3DMVP/">
                          <papertitle>3D-MVP: 3D Multiview Pretraining for Robotic Manipulation</papertitle>
                      </a>
                      <br>
                      Shengyi Qian, Kaichun Mo, Valts Blukis, David F. Fouhey, Dieter Fox, <strong>Ankit Goyal</strong>
                      <br>
                      <em>CVPR 2025</em> 
                      <br>
                      <!-- <a href="https://github.com/jasonqsy/3DMVP">[code]</a>  -->
                      <a href="https://jasonqsy.github.io/3DMVP/">[project page]</a> 
                      <a href="https://arxiv.org/abs/2406.18158">[paper]</a>
                      <p align="justify">
                        We propose 3D multi-view pretraining using MAEs for robot manipulation. 
                      </p>
                  </p>
              </td>
          </tr>

	  <!-- RVT-2 -->
          <tr class="publication-row">
              <td width="35%">
                  <img src='https://robotic-view-transformer-2.github.io/figs/teaser.gif' width=100%>
              </td>
              <td valign="top" width="70%">
                  <p>
                      <a href="https://robotic-view-transformer-2.github.io/">
                          <papertitle>RVT-2: Learning Precise Manipulation from Few Examples</papertitle>
                      </a>
                      <br>
                      <strong>Ankit Goyal</strong>, Valts Blukis, Jie Xu, Yijie Guo, Yu-Wei Chao, Dieter Fox
                      <br>
                      <em>RSS 2024</em> 
                      <br>
                      <a href="https://github.com/nvlabs/rvt">[code]</a> 
                      <a href="https://robotic-view-transformer-2.github.io/">[project page]</a> 
                      <p align="justify">
                          We study how to build a robotic system that can solve high-precision manipulation tasks from a few demonstrations. Prior works, like PerAct and RVT, have studied few-shot manipulation; however, they often struggle with tasks requiring high precision. We study how to make them more effective, precise, and fast. Using a combination of architectural and system-level improvements, we propose RVT-2, a multitask 3D manipulation model that is 6X faster in training and 2X faster in inference than its predecessor RVT. RVT-2 achieves a new state-of-the-art on RLBench.
                      </p>
                  </p>
              </td>
          </tr>

	  <!-- RVT -->
          <tr class="publication-row">
            <td width="35%">
              <img src='https://robotic-view-transformer.github.io/real_world/real_world_small.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://robotic-view-transformer.github.io/">
                  <papertitle>RVT: Robotic View Transformer for 3D Object Manipulation</papertitle>
                </a>
                <br>
		            <strong>Ankit Goyal</strong>, Jie Xu, Yijie Guo, Valts Blukis, Yu-Wei Chao, Dieter Fox
                <br>
		<em>CoRL 2023 (Oral)</em> 
		<br>
		<a href="https://github.com/nvlabs/rvt">[code]</a> 
		<a href="https://robotic-view-transformer.github.io/">[project page]</a> 
		<a href="https://www.youtube.com/watch?v=mIQN4f3KSA8">[video]</a> 
		<a href="data/rvt.key">[slides]</a> 


		<p align="justify"> RVT is a multi-view transformer for 3D manipulation that is both scalable and accurate.  In simulations, a single RVT model works well across 18 RLBench tasks with 249 task variations, achieving 26% higher relative success than existing  SOTA (PerAct). It also trains 36X faster than PerAct for achieving the same performance and achieves 2.3X the inference speed of PerAct. Further, RVT can perform a variety of manipulation tasks in the real world with just a few (~10) demonstrations per task.</p>
            </td>
          </tr>

	  <!-- RPDiff -->
          <tr class="publication-row">
            <td width="35%">
		    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="https://anthonysimeonov.github.io/rpdiff-multi-modal/img/book_bookshelf_seq1_8x_rdc_lout.mp4" type="video/mp4">
                </video>
              <!-- <iframe src="https://anthonysimeonov.github.io/rpdiff-multi-modal/img/book_bookshelf_seq1_8x_rdc_lout.mp4" -->
              <!--     frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
              <!-- <img src='https://anthonysimeonov.github.io/rpdiff-multi-modal/img/book_bookshelf_seq1_8x_rdc_lout.mp4' width=100%> -->
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://anthonysimeonov.github.io/rpdiff-multi-modal/">
                  <papertitle>Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement</papertitle>
                </a>
                <br>
		Anthony Simeonov, Ankit Goyal, Lucas Manuelli, Lin Yen-Chen, Alina Sarmiento, Alberto Rodriguez, Pulkit Agrawal, Dieter Fox
                <br>
		<em>CoRL 2023</em> 
		<br>
		<a href="https://github.com/anthonysimeonov/rpdiff">[code]</a> 
		<a href="https://anthonysimeonov.github.io/rpdiff-multi-modal/">[project page]</a> 
		<a href="https://www.youtube.com/watch?v=x9noTl_aqu0">[video]</a> 
		<p align="justify"> RPDiff rearranges objects into "multimodal" configurations, such as a book inserted in an open slot of a bookshelf. It generalizes to novel geometries, poses, and layouts, and is trained from demonstrations to operate on point clouds.</p>
            </td>
          </tr>

	  <!-- Infinigen -->
          <tr class="publication-row">
            <td width="35%">
              <img src='images/infinigen.jpeg' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://infinigen.org/">
                  <papertitle>Infinite Photorealistic Worlds using Procedural Generation</papertitle>
                </a>
                <br>
		Alexander Raistrick, Lahav Lipson, Zeyu Ma, Lingjie Mei, Mingzhe Wang, Yiming Zuo, Karhan Kayan, Hongyu Wen, Beining Han, Yihan Wang, Alejandro Newell, Hei Law, <strong>Ankit Goyal</strong>, Kaiyu Yang, and Jia Deng
                <br>
		<em>CVPR 2023</em> 
		<br>
		<a href="https://infinigen.org/">[project page]</a> 
		<p align="justify">Infinigen is a generator of unlimited high-quality 3D data. Procedural and open-source.</p>
            </td>
          </tr>

	  <!-- ProgPrompt -->
          <tr class="publication-row">
            <td width="35%">
              <img src='images/progprompt.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2209.11302">
                  <papertitle>ProgPrompt: Generating Situated Robot Task Plans using Large Language Models</papertitle>
                </a>
                <br>
		Ishika Singh, Valts Blukis, Arsalan Mousavian, <strong>Ankit Goyal</strong>, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg 
                <br>
		<em>ICRA 2023</em> 
		<br>
		Also in <em>Autonomous Robots</em>, <em>LaRel @ NeurIPS 2022</em> and <em>LangRob @ CoRL 2022</em>
		<br>
		<a href="https://progprompt.github.io/">[project page]</a> 
		<p align="justify">We use large language models (LLMs) for task planning in robotics. We construct pythonic prompts, which specify the task, robot capabilities and the environment to seed LLMs.</p>
            </td>
          </tr>
	  <!-- IFOR -->

          <tr class="publication-row">
            <td width="35%">
              <img src='images/ifor.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2202.00732">
                  <papertitle>IFOR: Iterative Flow Minimization for Robotic Object Rearrangement</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Arsalan Mousavian, Chris Paxton, Yu-Wei Chao, Brian Okorn, Jia Deng, Dieter Fox
                <br>
		<em>CVPR 2022</em> 
		<br>
		Also in <em>EAI @ CVPR 2022</em>
		<br>
		<a href="https://imankgoyal.github.io/ifor.html">[project page]</a> 
		<a href="data/ifor_slides.pdf">[slides]</a> 
		<a href="data/ifor_poster.pdf">[poster]</a> 
		<p align="justify">IFOR is an end-to-end method for the challenging problem of object rearrangement for unknown objects given an RGBD image of the original and final scenes. It works on cluttered scenes in the real world, while training only on synthetic data. </p>
            </td>
          </tr>

	  <!-- 6DOF -->
          <tr class="publication-row">
            <td width="35%">
              <img src='images/6dof.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2204.12516">
                  <papertitle>Coupled Iterative Refinement for 6D Multi-Object Pose Estimation</papertitle>
                </a>
                <br>
                Lahav Lipson, Zachary Teed, <strong>Ankit Goyal</strong>, Jia Deng
                <br>
                <em>CVPR 2022</em>               
		<br>
		<a href="https://arxiv.org/abs/2204.12516">[paper]</a> <a href="https://github.com/princeton-vl/Coupled-Iterative-Refinement">[code]</a><br> 
		<p align="justify">We propose state-of-the-art 6DOF multi-object pose estimation system. Our system iteratively refines object pose and correspondece.</p>
            </td>
          </tr>

	  <!-- Non-deep Networks -->
          <tr class="publication-row">
            <td width="35%">
              <img src='images/nondeep.jpg' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2110.07641">
                  <papertitle>Non-deep Networks</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Alexey Bochkovskiy, Jia Deng, Vladlen Koltun
                <br>
                <em>NeurIPS 2022</em>                
		<br>
		<a href="https://github.com/imankgoyal/NonDeepNetworks">[code]</a> 
		<a href="data/non_deep_poster.pdf">[poster]</a> 
		<a href="data/non_deep_conf.key">[slides]</a> 
		<a href="https://neurips.cc/virtual/2022/poster/55098">[video]</a> 
		<p align="justify"> Depth is the hallmark of DNNs. But more depth means more sequential computation and higher latency. This begs the question -- is it possible to build high-performing ``non-deep" neural networks? We show it is. </p>
            </td>
          </tr>
	
	  <!-- SimpleView -->
          <tr class="publication-row">
            <td width="35%">
              <img src='images/simpleview.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2106.05304">
                  <papertitle>Revisiting Point Cloud Shape Classification with a Simple and Effective Baseline</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Hei Law, Bowei Liu, Alejnadro Newell, Jia Deng 
                <br>
                <em>ICML</em> 2021                
		<br>
                <a href="https://github.com/princeton-vl/SimpleView">[code]</a> 
		<a href="https://docs.google.com/presentation/d/1eYKDth19S2Ovt1lO0CpbzCM_hh7QEVni/edit?usp=drive_link&ouid=111606148346153000949&rtpof=true&sd=true">[slides]</a>
		<a href="https://drive.google.com/file/d/1TZIbcEaSQmjyOUiURYVzYthduzui1Rcx/view?usp=drive_link">[poster]</a>
		<a href="https://icml.cc/virtual/2021/poster/9099">[video]</a>
                <p align="justify"> Many point-based approaches have been proposed reporting steady benchmark improvements over time. We study the key ingredients of this progress and uncover two critical results. First, auxiliary factors, independent of the model architecture, make a large difference in performance. Second, a very simple projection based method performs surprisingly well. </p>
            </td>
          </tr>
          
          <!-- Rel3D -->
          <tr class="publication-row">
            <td width="35%">
              <img src='images/rel3D.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://papers.nips.cc/paper/2020/hash/76dc611d6ebaafc66cc0879c71b5db5c-Abstract.html">
                  <papertitle>Rel3D: A Minimally Contrastive Benchmark for Grounding Spatial Relations in 3D</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Kaiyu Yang, Dawei Yang, <a href = "http://www.cs.princeton.edu/~jiadeng/"> Jia Deng</a>
                <br>
                <em>NeuRIPS</em> 2020, <span style="color:brown;">Spotlight (Top 4% of submitted papers)</span>                
		<br>
                <a href="https://github.com/princeton-vl/Rel3D">[code]</a> 
		<a href="https://drive.google.com/file/d/1lWA2WlJR4itJJrnHMRiZ87-z8akagkH4/view?usp=sharing">[slides]</a>
		<a href="data/rel3d_poster.pdf">[poster]</a>
		<a href="https://slideslive.com/38935853/rel3d-a-minimally-contrastive-benchmark-or-grounding-spatial-relations-in-3d">[video]</a>
                <p align="justify"> Understanding spatial relations is important for both humans and robots. We create Rel3D, the first large-scale, human-annotated dataset for grounding spatial relations in 3D.  The 3D scenes in Rel3D come in minimally contrastive pairs: two scenes in a pair are almost identical, but a spatial relation holds in one and fails in the other. </p>
            </td>
          </tr>

          <!-- PackIt -->
          <tr class="publication-row">
            <td width="35%">
              <img src='images/packit.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2007.11121">
                  <papertitle>PackIt: A Virtual Environment for Geometric Planning</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, <a href = "http://www.cs.princeton.edu/~jiadeng/"> Jia Deng</a>
                <br>
                <em>ICML</em> 2020
                <br>
                <a href="https://github.com/princeton-vl/PackIt">[code]</a>
                <a href="data/packit_slides.pptx">[slides]</a>
		<a href="https://slideslive.com/38927501/packit-a-virtual-environment-for-geometric-planning?ref=search">[video]</a>
                <p align="justify"> Simultaneously reasoning about geometry and planning action is crucial for intelligent agents. This ability of geometric planning comes in handy while grocery shopping, rearranging room, warehouse management etc. We create PackIt, a virtual environment that caters to geometric planning.</p>
            </td>
          </tr>

          <!-- Think Visually -->
          <tr class="publication-row">
            <td width="35%">
              <img src='images/dsmn2.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/1805.11025">
                  <papertitle>Think Visually: Question Answering through Virtual Imagery</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Jian Wang, <a href = "http://www.cs.princeton.edu/~jiadeng/"> Jia Deng</a>
                <br>
                <em>ACL</em> 2018
                <br>
                <a href="https://github.com/princeton-vl/think_visually">[code]</a>
                <a href="data/think_visually_poster.pdf">[poster]</a>
                <p align="justify">We study geometric reasoning in the context of question-answering. We introduce Dynamic Spatial Memory Network (DSMN), a deep network architecture designed for answering questions that admit latent visual representations. </p>
            </td>
          </tr>

          <!-- ProtoNN -->
          <tr class="publication-row">
            <td width="35%">
              <img src='images/protonn.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <a href="http://proceedings.mlr.press/v70/gupta17a.html">
                <papertitle>ProtoNN: Compressed and Accurate kNN for Resource-scarce Devices</papertitle>
              </a>
              <br>
              C Gupta, AS Suggala,<strong> A Goyal</strong>, HV Simhadri, BP, AK, SG, RU, MV, <a href="https://www.microsoft.com/en-us/research/people/prajain/">P Jain</a>
              <br>
              <em>ICML</em> 2017
              <br>
              <a href="https://github.com/Microsoft/EdgeML">[code]</a>
              <br><br>
              <a href="https://patents.google.com/patent/US20180330275A1/en">
                <papertitle>Resource-Efficient Machine Learning</papertitle>
              </a>
              <br>
              Prateek Jain, Chirag Gupta, AS Suggala,<strong> Ankit Goyal</strong>, HV Simhadri
              <br>
              <em>US Patent Applicaiton</em>
              <br>
              <p align="justify">We propose ProtoNN, a novel algorithm that addresses the problem of real-time and accurate prediction on resource-scarce devices. </p>
             
            </td>
          </tr>

          <!-- Emotion Prediction -->
          <tr class="publication-row">
            <td width="35%">
              <img src='images/multimodal.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <a href="https://ieeexplore.ieee.org/abstract/document/7472192/">
                <papertitle>A Multimodal Mixture-Of-Experts Model for Dynamic Emotion Prediction in Movies</papertitle>
              </a>
              <br>
              <strong>Ankit Goyal</strong>, <a href="https://algoseer.github.io/">Naveen Kumar</a>, <a href="http://tanayag.com/Home.html">Tanaya Guha</a>, <a href="https://sail.usc.edu/people/shri.php">Shrikanth S. Narayanan</a>
              <br>
              <em>ICASSP</em> 2016
              <br>
              <p align="justify">We address the problem of continuous emotion prediction in movies. We propose a Mixture of Experts (MoE)-based fusion model that dynamically combines information from the audio and video modalities for predicting the emotion evoked in movies. </p>
              
            </td>
          </tr>

          <!-- <!-1- SURF -1-> -->
          <!-- <tr> -->
          <!--   <td width="35%"> -->
          <!--     <img src='images/surf.png' width=100%> -->
          <!--   </td> -->
          <!--   <td valign="top" width="70%"> -->
          <!--     <a href="https://link.springer.com/chapter/10.1007/978-3-319-27000-5_34"> -->
          <!--       <papertitle>Object Matching Using Speeded Up Robust Features</papertitle> -->
          <!--     </a> -->
          <!--     <br> -->
          <!--     NK Verma, <strong> Ankit Goyal</strong>, A Harsha Vardhan, Rahul Kumar Sevakula, Al Salour -->
          <!--     <br> -->
          <!--     <em>IES</em> 2016 -->
          <!--     <br> -->
          <!--     <p align="justify"> We propose a robust algorithm which is capable of detecting all the instances of a particular object in a scene image using Speeded Up Robust Features. </p> -->
          <!--   </td> -->
          <!-- </tr> -->

          <!-- <!-1- Template Matching -1-> -->
          <!-- <tr> -->
          <!--   <td width="35%"> -->
          <!--     <img src='images/template.png' width=100%> -->
          <!--   </td> -->
          <!--   <td valign="top" width="70%"> -->
          <!--     <p> -->
          <!--       <a href="https://ieeexplore.ieee.org/abstract/document/7334132/"> -->
          <!--         <papertitle>Template Matching for Inventory Management using Fuzzy Color Histogram and Spatial Filters</papertitle> -->
          <!--       </a> -->
          <!--       <br> -->
          <!--       NK Verma,<strong> Ankit Goyal</strong>, Anadi Chaman, Rahul K Sevakula, Al Salour -->
          <!--       <br> -->
          <!--       <em>ICIEA</em> 2015 -->
          <!--       <br> -->
          <!--       <p align="justify">We propose a methodology for object counting using color histogram based segmentation and spatial filters. </p> -->
          <!--    </p> --> 
          <!--   </td> -->
          <!-- </tr> -->

        </table>

        <!-- Media Coverage -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td width="100%" valign="middle">
        <heading>Media Coverage</heading>
      </td>
    </tr>
        </table>

<div class="media-bricks">
          <!-- Featured Coverage - Row 1 -->
          <a href="https://cvpr.thecvf.com/Conferences/2025/News/AI_Enhanced_Robotics" class="media-brick-item">
            <div class="media-brick-box" style="background-color: #1a5fb4;">
              <span>CVPR News</span>
            </div>
            <div class="media-brick-paper">3D-MVP</div>
            <div class="media-brick-desc">Featured in CVPR 2025 highlights.</div>
          </a>
          <a href="https://blogs.nvidia.com/blog/ai-research-iclr-2025/" class="media-brick-item">
            <div class="media-brick-box" style="background-color: #76B900;">
              <span>NVIDIA Blog</span>
            </div>
            <div class="media-brick-paper">HAMSTER</div>
            <div class="media-brick-desc">HAMSTER research at ICLR 2025.</div>
          </a>
          <a href="https://news.mit.edu/2025/new-system-enables-robots-to-solve-manipulation-problems-seconds-0605" class="media-brick-item">
            <div class="media-brick-box" style="background-color: #A31F34;">
              <span>MIT News</span>
            </div>
            <div class="media-brick-paper">cuTAMP</div>
            <div class="media-brick-desc">Solve manipulation in seconds.</div>
          </a>
          <a href="https://www.therobotreport.com/nvidias-rvt-can-learn-new-tasks-after-just-10-demos/" class="media-brick-item">
            <div class="media-brick-box" style="background-color: #0891b2;">
              <span>Robot Report</span>
            </div>
            <div class="media-brick-paper">RVT</div>
            <div class="media-brick-desc">Learn tasks after just 10 demos.</div>
          </a>
          <!-- Row 2 -->
          <a href="https://news.ycombinator.com/item?id=36376071" class="media-brick-item">
            <div class="media-brick-box" style="background-color: #ff6600;">
              <span>Hacker News</span>
            </div>
            <div class="media-brick-paper">Infinigen</div>
            <div class="media-brick-desc">Featured on front page.</div>
          </a>
          <a href="https://towardsdatascience.com/non-deep-networks-b0b80c65c7c6/" class="media-brick-item">
            <div class="media-brick-box" style="background-color: #047857;">
              <span>Towards Data Science</span>
            </div>
            <div class="media-brick-paper">Non-Deep Networks</div>
            <div class="media-brick-desc">"Less is the new more?"</div>
          </a>
          <a href="https://www.hackster.io/news/rvt-2-is-a-fast-learner-0469d3578acf" class="media-brick-item">
            <div class="media-brick-box" style="background-color: #2e7d32;">
              <span>Hackster.io</span>
            </div>
            <div class="media-brick-paper">RVT-2</div>
            <div class="media-brick-desc">RVT-2 is a fast learner.</div>
          </a>
          <a href="https://medium.com/ai-disruption/nvidia-releases-general-robot-model-rvt-2-with-6x-training-efficiency-boost-55b61a1e558b" class="media-brick-item">
            <div class="media-brick-box" style="background-color: #000000;">
              <span>Medium</span>
            </div>
            <div class="media-brick-paper">RVT-2</div>
            <div class="media-brick-desc">6X training efficiency boost.</div>
          </a>
        </div>

          <!-- Collapsible Other Coverage -->
        <div style="padding: 0 20px; margin-top: 15px;">
              <details class="details-example">
                <summary>Other coverage</summary>
            <div class="media-bricks" style="margin-top: 15px;">
              <a href="https://blog.csdn.net/nenchoumi3119/article/details/153961176" class="media-brick-item">
                <div class="media-brick-box" style="background-color: #FC5531;">
                  <span>CSDN</span>
                        </div>
                <div class="media-brick-paper">VLA-0</div>
                <div class="media-brick-desc">VLA paper review.</div>
              </a>
              <a href="https://www.moomoo.com/news/post/47625914/nvidia-releases-the-general-robot-model-rvt-2-accelerating-the" class="media-brick-item">
                <div class="media-brick-box" style="background-color: #ff9500;">
                  <span>Moomoo</span>
                </div>
                <div class="media-brick-paper">RVT-2</div>
                <div class="media-brick-desc">Accelerating robotics revolution.</div>
              </a>
              <a href="https://www.pnprobotics.com/nd.jsp?id=186" class="media-brick-item">
                <div class="media-brick-box" style="background-color: #0066CC;">
                  <span>PNP Robotics</span>
                </div>
                <div class="media-brick-paper">RVT-2</div>
                <div class="media-brick-desc">6X training efficiency.</div>
              </a>
              <a href="https://blog.csdn.net/s_m_c/article/details/141253235" class="media-brick-item">
                <div class="media-brick-box" style="background-color: #FC5531;">
                  <span>CSDN</span>
                </div>
                <div class="media-brick-paper">RVT-2</div>
                <div class="media-brick-desc">Technical analysis.</div>
              </a>
              <a href="https://blog.csdn.net/yorkhunter/article/details/142838556" class="media-brick-item">
                <div class="media-brick-box" style="background-color: #FC5531;">
                  <span>CSDN</span>
                </div>
                <div class="media-brick-paper">RVT-2</div>
                <div class="media-brick-desc">Precise manipulation.</div>
              </a>
              <a href="https://news.futunn.com/en/post/51689268/nvidia-releases-the-general-robot-model-rvt-2-accelerating-the" class="media-brick-item">
                <div class="media-brick-box" style="background-color: #00A870;">
                  <span>Futu News</span>
                </div>
                <div class="media-brick-paper">RVT-2</div>
                <div class="media-brick-desc">Robotics revolution.</div>
              </a>
              <a href="https://www.hackster.io/news/a-new-perspective-on-3d-object-manipulation-7d912844bc73" class="media-brick-item">
                <div class="media-brick-box" style="background-color: #2e7d32;">
                  <span>Hackster.io</span>
                </div>
                <div class="media-brick-paper">RVT</div>
                <div class="media-brick-desc">3D object manipulation.</div>
              </a>
              <a href="https://marktechpost-newsletter.beehiiv.com/p/ai-news-baidu-challenges-gpt-theorem-proving-llms-motiongpt-ai-updates-june-28-2023-edition" class="media-brick-item">
                <div class="media-brick-box" style="background-color: #7c3aed;">
                  <span>MarkTechPost</span>
                        </div>
                <div class="media-brick-paper">RVT</div>
                <div class="media-brick-desc">AI news weekly update.</div>
              </a>
              <a href="https://quantumzeitgeist.com/bridging-simulation-and-reality-behavior-cloning-for-real-world-manipulation-tasks/" class="media-brick-item">
                <div class="media-brick-box" style="background-color: #1e40af;">
                  <span>Quantum Zeitgeist</span>
                        </div>
                <div class="media-brick-paper">RVT</div>
                <div class="media-brick-desc">Sim-to-real transfer.</div>
              </a>
              <a href="https://sh-tsang.medium.com/brief-review-parnet-non-deep-networks-e2e1d0d243aa" class="media-brick-item">
                <div class="media-brick-box" style="background-color: #000000;">
                  <span>Medium</span>
                        </div>
                <div class="media-brick-paper">Non-Deep Networks</div>
                <div class="media-brick-desc">ParNet brief review.</div>
              </a>
              <a href="https://syncedreview.com/2021/10/22/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-129/" class="media-brick-item">
                <div class="media-brick-box" style="background-color: #3b82f6;">
                  <span>Synced</span>
                        </div>
                <div class="media-brick-paper">Non-Deep Networks</div>
                <div class="media-brick-desc">High-performance ML.</div>
              </a>
            </div>
              </details>
        </div>

        <!-- Open-Source Code -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td width="100%" valign="middle">
        <heading>Open-Source Code</heading>
      </td>
    </tr>
        </table>

        <div class="media-bricks">
          <a href="https://github.com/NVlabs/vla0" class="media-brick-item">
            <div class="media-brick-box" style="background-color: #24292e;">
              <span>VLA-0</span>
            </div>
            <div class="media-brick-paper"><img src="https://img.shields.io/github/stars/NVlabs/vla0?style=flat-square&color=1772d0" alt="Stars"> <img src="https://img.shields.io/github/forks/NVlabs/vla0?style=flat-square&color=888" alt="Forks"></div>
            <div class="media-brick-desc">Vision-Language-Action models.</div>
          </a>
          <a href="https://github.com/NVlabs/RVT" class="media-brick-item">
            <div class="media-brick-box" style="background-color: #24292e;">
              <span>RVT & RVT-2</span>
            </div>
            <div class="media-brick-paper"><img src="https://img.shields.io/github/stars/NVlabs/RVT?style=flat-square&color=1772d0" alt="Stars"> <img src="https://img.shields.io/github/forks/NVlabs/RVT?style=flat-square&color=888" alt="Forks"></div>
            <div class="media-brick-desc">Robotic View Transformer.</div>
          </a>
          <a href="https://github.com/imankgoyal/NonDeepNetworks" class="media-brick-item">
            <div class="media-brick-box" style="background-color: #24292e;">
              <span>Non-Deep Networks</span>
            </div>
            <div class="media-brick-paper"><img src="https://img.shields.io/github/stars/imankgoyal/NonDeepNetworks?style=flat-square&color=1772d0" alt="Stars"> <img src="https://img.shields.io/github/forks/imankgoyal/NonDeepNetworks?style=flat-square&color=888" alt="Forks"></div>
            <div class="media-brick-desc">Parallel neural networks.</div>
          </a>
          <a href="https://github.com/princeton-vl/SimpleView" class="media-brick-item">
            <div class="media-brick-box" style="background-color: #24292e;">
              <span>SimpleView</span>
            </div>
            <div class="media-brick-paper"><img src="https://img.shields.io/github/stars/princeton-vl/SimpleView?style=flat-square&color=1772d0" alt="Stars"> <img src="https://img.shields.io/github/forks/princeton-vl/SimpleView?style=flat-square&color=888" alt="Forks"></div>
            <div class="media-brick-desc">Point cloud classification.</div>
          </a>
          <a href="https://github.com/princeton-vl/PackIt" class="media-brick-item">
            <div class="media-brick-box" style="background-color: #24292e;">
              <span>PackIt</span>
            </div>
            <div class="media-brick-paper"><img src="https://img.shields.io/github/stars/princeton-vl/PackIt?style=flat-square&color=1772d0" alt="Stars"> <img src="https://img.shields.io/github/forks/princeton-vl/PackIt?style=flat-square&color=888" alt="Forks"></div>
            <div class="media-brick-desc">Geometric planning environment.</div>
          </a>
          <a href="https://github.com/princeton-vl/Rel3D" class="media-brick-item">
            <div class="media-brick-box" style="background-color: #24292e;">
              <span>Rel3D</span>
            </div>
            <div class="media-brick-paper"><img src="https://img.shields.io/github/stars/princeton-vl/Rel3D?style=flat-square&color=1772d0" alt="Stars"> <img src="https://img.shields.io/github/forks/princeton-vl/Rel3D?style=flat-square&color=888" alt="Forks"></div>
            <div class="media-brick-desc">Spatial relations in 3D.</div>
          </a>
        </div>

        <!-- Patents -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td width="100%" valign="middle">
        <heading>Patents</heading>
      </td>
    </tr>
        </table>

        <div class="media-bricks">
          <a href="https://patents.google.com/patent/US12260630B2/en" class="media-brick-item">
            <div class="media-brick-box" style="background: linear-gradient(135deg, #76B900, #5a8f00);">
              <span>GRANTED</span>
                </div>
            <div class="media-brick-paper">Non-Deep Networks</div>
            <div class="media-brick-desc">Parallel architectures for NN.</div>
          </a>
          <a href="https://patents.google.com/patent/US20240273810A1/en" class="media-brick-item">
            <div class="media-brick-box" style="background: linear-gradient(135deg, #95a5a6, #7f8c8d);">
              <span>PENDING</span>
                </div>
            <div class="media-brick-paper">RVT</div>
            <div class="media-brick-desc">View transformation for 3D.</div>
          </a>
          <a href="https://patents.google.com/patent/US20240371082A1/en" class="media-brick-item">
            <div class="media-brick-box" style="background: linear-gradient(135deg, #95a5a6, #7f8c8d);">
              <span>PENDING</span>
                </div>
            <div class="media-brick-paper">RVT-2</div>
            <div class="media-brick-desc">Multi-stage 3D inference.</div>
          </a>
          <a href="https://patents.google.com/patent/US20230234233A1/en" class="media-brick-item">
            <div class="media-brick-box" style="background: linear-gradient(135deg, #95a5a6, #7f8c8d);">
              <span>PENDING</span>
                </div>
            <div class="media-brick-paper">IFOR</div>
            <div class="media-brick-desc">Object placement with NN.</div>
          </a>
<a href="https://patents.google.com/patent/US20240095077A1/en" class="media-brick-item">
            <div class="media-brick-box" style="background: linear-gradient(135deg, #95a5a6, #7f8c8d);">
              <span>PENDING</span>
                </div>
            <div class="media-brick-paper">ProgPrompt</div>
            <div class="media-brick-desc">Prompt generator for ML.</div>
          </a>
          <a href="https://patents.google.com/patent/US20250381667A1/en" class="media-brick-item">
            <div class="media-brick-box" style="background: linear-gradient(135deg, #95a5a6, #7f8c8d);">
              <span>PENDING</span>
                </div>
            <div class="media-brick-paper">3D-MVP</div>
            <div class="media-brick-desc">Multi-view pretraining for robots.</div>
          </a>
          <a href="https://patents.google.com/patent/US20250156764A1/en" class="media-brick-item">
            <div class="media-brick-box" style="background: linear-gradient(135deg, #95a5a6, #7f8c8d);">
              <span>PENDING</span>
                </div>
            <div class="media-brick-paper">Ada-demo</div>
            <div class="media-brick-desc">Adaptive demonstrations.</div>
          </a>
        </div>

        <!-- Teaching -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td width="100%" valign="middle">
                <heading>Teaching</heading>
              </td>
            </tr>
        </table>

        <div class="media-bricks">
          <div class="media-brick-item">
            <div class="media-brick-box" style="background-color: #e07020;">
              <span>Princeton</span>
            </div>
            <div class="media-brick-paper">COS529</div>
            <div class="media-brick-desc">Advanced CV [Winter 2020]</div>
          </div>
          <div class="media-brick-item">
            <div class="media-brick-box" style="background-color: #e07020;">
              <span>Princeton</span>
            </div>
            <div class="media-brick-paper">COS429</div>
            <div class="media-brick-desc">Computer Vision [Fall 2018]</div>
          </div>
          <div class="media-brick-item">
            <div class="media-brick-box" style="background-color: #00274C;">
              <span>U-Michigan</span>
            </div>
            <div class="media-brick-paper">EECS442</div>
            <div class="media-brick-desc">Computer Vision [Fall 2017]</div>
          </div>
          <div class="media-brick-item">
            <div class="media-brick-box" style="background-color: #00274C;">
              <span>U-Michigan</span>
            </div>
            <div class="media-brick-paper">EECS442</div>
            <div class="media-brick-desc">Computer Vision [Winter 2018]</div>
          </div>
        </div>

        <!-- Reference -->
        <p align="right"><a href="https://jonbarron.info/">[Web Cite]</a></p>
      </td>
    </tr>
  </table>  

</body>

</html>
