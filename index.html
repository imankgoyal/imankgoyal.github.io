<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-129673183-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-129673183-1');
</script>

  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/icon.png">
  <title>Ankit Goyal</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <!-- Bio and Image -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="80%" valign="middle">
              <p align="center">
                <name>Ankit Goyal</name>
              </p>
	      <p>&nbsp;</p>
              <p align="justify">
		I am a Research Scientist in Robotics at NVIDIA working with <a href="https://homes.cs.washington.edu/~fox/"> Dieter Fox</a>. I did my Ph.D. in Computer Science at Princeton University, where I was advised by <a href = "http://www.cs.princeton.edu/~jiadeng/">Prof. Jia Deng</a>. I completed Masters from University of Michigan and Bachelors from IIT Kanpur. 
	      <p>
	      <!-- <p align="justify" style="color:red;"> -->
	      <!--   <b><span color="red"> --> 
		<!-- I am looking for full-time research positions starting from Summer/Fall 2022. -->
		<!-- </span></b> -->
	      <!-- </p> -->
	      <p align="justify"> 
	      	I have been fortunate to intern at some wonderful places and work with amazing mentors.
	        <ul>
	          <li> <i>Summer 2021:</i> Nvidia with <a href="https://homes.cs.washington.edu/~fox/"> Dieter Fox </a>
	          <li> <i>Winter 2021:</i> Intel with <a href="http://vladlen.info/"> Vladlen Koltun </a>
	          <li> <i>Summer 2016:</i> Microsoft Research India with <a href="https://www.prateekjain.org/">Prateek Jain</a>
	          <li> <i>Summer 2015:</i> USC with <a href="https://sail.usc.edu/people/shri.php">Shrikanth Narayanan</a>, <a href="http://tanayag.com/Home.html">Tanaya Guha</a> and <a herf="https://navkr.com/"> Naveen Kumar </a>
	        </ul>
              </p>
	      <p align="justify"> <strong> 
	        Recent News: </strong>
	        <ul>
		  <li> <i>October 2022:</i> <a href='https://github.com/princeton-vl/Coupled-Iterative-Refinement'>Coupled iterative refinement</a> led by <a href='https://www.lahavlipson.com/'> Lahav Lipson </a> won an award in ECCV BOP challenge 2022</b>.
		  <li> <i>September 2022:</i> Received the <a href='https://neurips.cc/Conferences/2022/FinancialAssistance'> NeurIPS Scholar Award</a></b>.
		  <li> <i>September 2022:</i> Paper on <a href="https://arxiv.org/abs/2110.07641">non-deep networks</a> accepted to <b>NeurIPS 2022</b>.
		  <li> <i>August 2022:</i> Defended my Ph.D. thesis and started as a Research Scientist at NVIDIA working with <a href="https://homes.cs.washington.edu/~fox/"> Dieter Fox</a> </b>.	
		  <li> <i>March 2022:</i> Two papers accepted to <b>CVPR 2022</b>.
	          <li> <i>August 2021:</i> Recognized as an <a href="http://iccv2021.thecvf.com/outstanding-reviewers">outstanding reviewer</a> at <b> ICCV 2021</b>.
		  <li> <i>July 2021:</i> Selected for <b>Qualcomm Innovation Fellowship</b> with Zachary Teed.
	        </ul>
              </p>
	    </td>
            <td width="20%">
              <img src="images/self.png">
            </td>
          </tr>
	  <tr>
            <td colspan="2">
              <p align=center margin=0px>
                <a href="mailto:agoyal@princeton.edu">Email</a> &nbsp/&nbsp
                <a href="data/AnkitGoyal_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=RhN6jKIAAAAJ&hl=en&oi=sra">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/imankgoyal">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/ankit-goyal-5baaa287/"> LinkedIn </a> &nbsp/&nbsp
	        <a href="https://twitter.com/imankitgoyal?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @imankitgoyal</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
             </p>
	     <p>&nbsp;</p>
	   </td>
	  </tr>
        </table>
	<div class = "row">
        <table width="100%" align="center" margin-top=100px>
            <tr>
              <td align="center" width="16%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/princeton.png" width="30%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/nvidia.png" width="50%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/intel.png" width="60%">
              </td>

              <td align="center" width="16%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/michigan.png" width="50%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/msr.png" width="90%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/usc.png" width="30%">
              </td>

              <td align="center" width="14%" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                <img src = "/images/iitk.png" width="60%">
              </td>
            </tr>

            <tr>
                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">PhD, CS<br>Princeton University<br>2018 - 2022</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Intern<br>NVIDIA<br>Summer-Fall 2021</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Intern<br>Intel<br>Winter 2021</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">MS, CSE<br>University of Michigan<br>2016 - 2018</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Intern<br>MSR<br>Summer 2016</td>
		
                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">Research Intern<br>USC<br>Summer 2015</td>

                <td align="center" style = "vertical-align: middle; background-color: rgba(255, 255, 255, 1)">BTech, EE<br>IIT Kanpur<br>2012 - 2016</td>
          </tr>

          </table>
      </div>

        <!-- Research Interest -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td width="100%" valign="middle">
        <heading>Research</heading>
        <p align="justify">
          I am interested in understanding various aspects of intelligence, especially reasoning and common sense. In particular, I want to develop computation models for various reasoning skills that humans possess.
        </p>
      </td>
    </tr>
        </table>

        <!-- Publications-->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	  <!-- IFOR -->
          <tr>
            <td width="35%">
              <img src='images/ifor.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2202.00732">
                  <papertitle>IFOR: Iterative Flow Minimization for Robotic Object Rearrangement</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Arsalan Mousavian, Chris Paxton, YWC, BO, JD, Dieter Fox
                <br>
		<em>CVPR 2022</em> 
		<br>
		Also in <em>EAI @ CVPR 2022</em>
		<br>
		<a href="https://imankgoyal.github.io/ifor.html">[webpage]</a> 
		<a href="data/ifor_slides.pdf">[slides]</a> 
		<a href="data/ifor_poster.pdf">[poster]</a> 
		<p align="justify">IFOR is an end-to-end method for the challenging problem of object rearrangement for unknown objects given an RGBD image of the original and final scenes. It works on cluttered scenes in the real world, while training only on synthetic data. </p>
            </td>
          </tr>

	  <!-- 6DOF -->
          <tr>
            <td width="35%">
              <img src='images/6dof.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2204.12516">
                  <papertitle>Coupled Iterative Refinement for 6D Multi-Object Pose Estimation</papertitle>
                </a>
                <br>
                Lahav Lipson, Zachary Teed, <strong>Ankit Goyal</strong>, Jia Deng
                <br>
                <em>CVPR 2022</em>               
		<br>
		<a href="https://arxiv.org/abs/2204.12516">[paper]</a> <a href="https://github.com/princeton-vl/Coupled-Iterative-Refinement">[code]</a><br> 
		<p align="justify">We propose state-of-the-art 6DOF multi-object pose estimation system. Our system iteratively refines object pose and correspondece.</p>
            </td>
          </tr>

	  <!-- Non-deep Networks -->
          <tr>
            <td width="35%">
              <img src='images/nondeep.jpg' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2110.07641">
                  <papertitle>Non-deep Networks</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Alexey Bochkovskiy, Jia Deng, Vladlen Koltun
                <br>
                <em>NeurIPS 2022</em>                
		<br>
		<a href="https://github.com/imankgoyal/NonDeepNetworks">[code]</a> 
		<a href="data/non_deep_poster.pdf">[poster]</a> 
		<a href="data/non_deep_conf.key">[slides]</a> 
		<a href="https://neurips.cc/virtual/2022/poster/55098">[video]</a> 
		<br> <br>
                <a href="https://syncedreview.com/2021/10/22/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-129/"><img src='images/synced.png' height=25px></a> &nbsp; &nbsp;
                <a href="https://papersread.ai/e/non-deep-networks/"><img src='images/podbean.png' height=25px></a>                 
		<p align="justify"> Depth is the hallmark of DNNs. But more depth means more sequential computation and higher latency. This begs the question -- is it possible to build high-performing ``non-deep" neural networks? We show it is. </p>
            </td>
          </tr>
	
	  <!-- SimpleView -->
          <tr>
            <td width="35%">
              <img src='images/simpleview.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2106.05304">
                  <papertitle>Revisiting Point Cloud Shape Classification with a Simple and Effective Baseline</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Hei Law, Bowei Liu, Alejnadro Newell, Jia Deng 
                <br>
                <em>ICML</em> 2021                
		<br>
                <a href="https://github.com/princeton-vl/SimpleView">[code]</a> 
		<a href="https://docs.google.com/presentation/d/1iYC5vDQPsb9nG9QCLwpm5TPL8i1F3sxu/edit?usp=sharing&ouid=116890615761823045471&rtpof=true&sd=true">[slides]</a>
		<a href="https://drive.google.com/file/d/1kn6qXDgLa48BcyzGr4-vheFP1CQtYXH6/view?usp=sharing">[poster]</a>
		<a href="https://icml.cc/virtual/2021/poster/9099">[video]</a>
                <p align="justify"> Many point-based approaches have been proposed reporting steady benchmark improvements over time. We study the key ingredients of this progress and uncover two critical results. First, auxiliary factors, independent of the model architecture, make a large difference in performance. Second, a very simple projection based method performs surprisingly well. </p>
            </td>
          </tr>
          
          <!-- Rel3D -->
          <tr>
            <td width="35%">
              <img src='images/rel3D.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://papers.nips.cc/paper/2020/hash/76dc611d6ebaafc66cc0879c71b5db5c-Abstract.html">
                  <papertitle>Rel3D: A Minimally Contrastive Benchmark for Grounding Spatial Relations in 3D</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Kaiyu Yang, Dawei Yang, <a href = "http://www.cs.princeton.edu/~jiadeng/"> Jia Deng</a>
                <br>
                <em>NeuRIPS</em> 2020, <span style="color:brown;">Spotlight (Top 4% of submitted papers)</span>                
		<br>
                <a href="https://github.com/princeton-vl/Rel3D">[code]</a> 
		<a href="https://drive.google.com/file/d/1lWA2WlJR4itJJrnHMRiZ87-z8akagkH4/view?usp=sharing">[slides]</a>
		<a href="data/rel3d_poster.pdf">[poster]</a>
		<a href="https://slideslive.com/38935853/rel3d-a-minimally-contrastive-benchmark-or-grounding-spatial-relations-in-3d">[video]</a>
                <p align="justify"> Understanding spatial relations is important for both humans and robots. We create Rel3D, the first large-scale, human-annotated dataset for grounding spatial relations in 3D.  The 3D scenes in Rel3D come in minimally contrastive pairs: two scenes in a pair are almost identical, but a spatial relation holds in one and fails in the other. </p>
            </td>
          </tr>

          <!-- PackIt -->
          <tr>
            <td width="35%">
              <img src='images/packit.gif' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/2007.11121">
                  <papertitle>PackIt: A Virtual Environment for Geometric Planning</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, <a href = "http://www.cs.princeton.edu/~jiadeng/"> Jia Deng</a>
                <br>
                <em>ICML</em> 2020
                <br>
                <a href="https://github.com/princeton-vl/PackIt">[code]</a>
                <a href="data/packit_slides.pptx">[slides]</a>
		<a href="https://slideslive.com/38927501/packit-a-virtual-environment-for-geometric-planning?ref=search">[video]</a>
                <p align="justify"> Simultaneously reasoning about geometry and planning action is crucial for intelligent agents. This ability of geometric planning comes in handy while grocery shopping, rearranging room, warehouse management etc. We create PackIt, a virtual environment that caters to geometric planning.</p>
            </td>
          </tr>

          <!-- Think Visually -->
          <tr>
            <td width="35%">
              <img src='images/dsmn2.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://arxiv.org/abs/1805.11025">
                  <papertitle>Think Visually: Question Answering through Virtual Imagery</papertitle>
                </a>
                <br>
                <strong>Ankit Goyal</strong>, Jian Wang, <a href = "http://www.cs.princeton.edu/~jiadeng/"> Jia Deng</a>
                <br>
                <em>ACL</em> 2018
                <br>
                <a href="https://github.com/princeton-vl/think_visually">[code]</a>
                <a href="data/think_visually_poster.pdf">[poster]</a>
                <p align="justify">We study geometric reasoning in the context of question-answering. We introduce Dynamic Spatial Memory Network (DSMN), a deep network architecture designed for answering questions that admit latent visual representations. </p>
            </td>
          </tr>

          <!-- ProtoNN -->
          <tr>
            <td width="35%">
              <img src='images/protonn.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <a href="http://proceedings.mlr.press/v70/gupta17a.html">
                <papertitle>ProtoNN: Compressed and Accurate kNN for Resource-scarce Devices</papertitle>
              </a>
              <br>
              C Gupta, AS Suggala,<strong> A Goyal</strong>, HV Simhadri, BP, AK, SG, RU, MV, <a href="https://www.microsoft.com/en-us/research/people/prajain/">P Jain</a>
              <br>
              <em>ICML</em> 2017
              <br>
              <a href="https://github.com/Microsoft/EdgeML">[code]</a>
              <br><br>
              <a href="https://patents.google.com/patent/US20180330275A1/en">
                <papertitle>Resource-Efficient Machine Learning</papertitle>
              </a>
              <br>
              Prateek Jain, Chirag Gupta, AS Suggala,<strong> Ankit Goyal</strong>, HV Simhadri
              <br>
              <em>US Patent Applicaiton</em>
              <br>
              <p align="justify">We propose ProtoNN, a novel algorithm that addresses the problem of real-time and accurate prediction on resource-scarce devices. </p>
             
            </td>
          </tr>

          <!-- Emotion Prediction -->
          <tr>
            <td width="35%">
              <img src='images/multimodal.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <a href="https://ieeexplore.ieee.org/abstract/document/7472192/">
                <papertitle>A Multimodal Mixture-Of-Experts Model for Dynamic Emotion Prediction in Movies</papertitle>
              </a>
              <br>
              <strong>Ankit Goyal</strong>, <a href="https://algoseer.github.io/">Naveen Kumar</a>, <a href="http://tanayag.com/Home.html">Tanaya Guha</a>, <a href="https://sail.usc.edu/people/shri.php">Shrikanth S. Narayanan</a>
              <br>
              <em>ICASSP</em> 2016
              <br>
              <p align="justify">We address the problem of continuous emotion prediction in movies. We propose a Mixture of Experts (MoE)-based fusion model that dynamically combines information from the audio and video modalities for predicting the emotion evoked in movies. </p>
              
            </td>
          </tr>

          <!-- SURF -->
          <tr>
            <td width="35%">
              <img src='images/surf.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <a href="https://link.springer.com/chapter/10.1007/978-3-319-27000-5_34">
                <papertitle>Object Matching Using Speeded Up Robust Features</papertitle>
              </a>
              <br>
              NK Verma, <strong> Ankit Goyal</strong>, A Harsha Vardhan, Rahul Kumar Sevakula, Al Salour
              <br>
              <em>IES</em> 2016
              <br>
              <p align="justify"> We propose a robust algorithm which is capable of detecting all the instances of a particular object in a scene image using Speeded Up Robust Features. </p>
            </td>
          </tr>

          <!-- Template Matching -->
          <tr>
            <td width="35%">
              <img src='images/template.png' width=100%>
            </td>
            <td valign="top" width="70%">
              <p>
                <a href="https://ieeexplore.ieee.org/abstract/document/7334132/">
                  <papertitle>Template Matching for Inventory Management using Fuzzy Color Histogram and Spatial Filters</papertitle>
                </a>
                <br>
                NK Verma,<strong> Ankit Goyal</strong>, Anadi Chaman, Rahul K Sevakula, Al Salour
                <br>
                <em>ICIEA</em> 2015
                <br>
                <p align="justify">We propose a methodology for object counting using color histogram based segmentation and spatial filters. </p>
             </p> 
            </td>
          </tr>
        </table>

        <!-- Teaching -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td width="100%" valign="middle">
                <heading>Teaching</heading>
                <p>
                  I have been a Teaching Assistant for the following courses:
                    <ul>
                      <li> COS529: Advanced Computer Vision at Princeton University [Winter 2020] </li>
                      <li> COS429: Computer Vision at Princeton University [Fall 2018] </li>
                      <li> EECS442: Computer Vision at University of Michigan  [Fall 2017, Winter 2018]</li>
                    </ul>
                </p>
<!-- 		Some <a href="https://imankgoyal.github.io/related.html">related papers</a> to mine. -->
              </td>
            </tr>
        </table>

        <!-- Reference -->
        <p align="right"><a href="https://jonbarron.info/">[Web Cite]</a></p>
      </td>
    </tr>
  </table>  

</body>

</html>
